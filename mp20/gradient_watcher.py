import os
import torch
import datetime

class GradientWatcher:
    """
    è‡ªåŠ¨ç›‘æ§ PyTorch æ¨¡å‹çš„æ¢¯åº¦çŠ¶å†µï¼š
    - æ£€æµ‹ NaN / Inf / çˆ†ç‚¸æ¢¯åº¦
    - æ‰“å°è­¦å‘Šå¹¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    """

    def __init__(self, model, threshold=50.0, log_path="grad_log.txt", verbose=True):
        """
        Args:
            model: nn.Module å¯¹è±¡
            threshold: è¶…è¿‡æ­¤å€¼æ‰“å°è­¦å‘Šï¼ˆé»˜è®¤ 50ï¼‰
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ grad_log.txtï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.model = model
        self.threshold = threshold
        self.verbose = verbose
        self.log_path = log_path
        self.handles = []
        self._create_log_file()
        self.register_hooks()

    def _create_log_file(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
        os.makedirs(os.path.dirname(self.log_path), exist_ok=True) if "/" in self.log_path else None
        with open(self.log_path, "w") as f:
            f.write(f"Gradient Watcher Log â€” {datetime.datetime.now()}\n")
            f.write("=" * 80 + "\n")

    def _log(self, message):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        with open(self.log_path, "a") as f:
            f.write(message + "\n")

    def register_hooks(self):
        """ä¸ºæ¯ä¸ªå¯è®­ç»ƒå‚æ•°æ³¨å†Œæ¢¯åº¦ hook"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                handle = param.register_hook(self._make_hook(name))
                self.handles.append(handle)

    def _make_hook(self, name):
        def hook(grad):
            timestamp = datetime.datetime.now().strftime("%H:%M:%S")
            # æ£€æŸ¥ NaN / Inf
            if torch.isnan(grad).any():
                msg = f"[{timestamp}] ğŸš¨ NaN detected in gradient of [{name}]"
                print(msg)
                self._log(msg)
            elif torch.isinf(grad).any():
                msg = f"[{timestamp}] ğŸš¨ Inf detected in gradient of [{name}]"
                print(msg)
                self._log(msg)

            # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
            grad_abs_max = grad.abs().max().item()
            grad_norm = grad.norm(2).item()
            if grad_abs_max > self.threshold:
                msg = f"[{timestamp}] âš ï¸ Large grad in [{name}] | max={grad_abs_max:.2f}, norm={grad_norm:.2f}"
                # print(msg)
                self._log(msg)

            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            if self.verbose and grad_abs_max <= self.threshold:
                msg = f"[{timestamp}] Grad OK in [{name}] | max={grad_abs_max:.2f}, norm={grad_norm:.2f}"
                self._log(msg)

            return grad
        return hook

    def remove(self):
        """ç§»é™¤æ‰€æœ‰ hook"""
        for handle in self.handles:
            handle.remove()
        self.handles.clear()
        print("âœ… GradientWatcher hooks removed.")

