import os
import json
import torch
import torch.distributed as dist
import numpy as np
from torch.nn.parallel import DistributedDataParallel as DDP
from torch_geometric.loader import DataLoader

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from src.data import ChunkedSmartDataset_h5, BinPackingSampler
from src.models import HTGPModel
from src.utils import HTGPConfig
from src.engine import PotentialTrainer 

# ==========================================
# 0. å…¨å±€ç¯å¢ƒè®¾ç½®
# ==========================================
torch.multiprocessing.set_sharing_strategy('file_system')
torch.set_default_dtype(torch.float32)
torch.backends.cuda.matmul.allow_tf32 = True 
torch.backends.cudnn.allow_tf32 = True

# ==========================================
# 1. è®­ç»ƒé…ç½®
# ==========================================
class Config:
    DATA_DIR = "/dev/shm/dataset_h5_r6_inorg"
    TRAIN_META = "train_metadata.pt"
    TEST_META = "test_metadata.pt"
    E0_PATH = "/dev/shm/dataset_h5_r6_inorg/meta_data.pt"
    LOG_DIR = "Checkpoints"

    MAX_COST_PER_BATCH = 10000 # 4000 cutoff ä¸º7
    LR = 1e-3
    EPOCHS = 100
    
    NUM_WORKERS = 8
    PREFETCH_FACTOR = 2

    # åŸºç¡€æ¨¡å‹å‚æ•° (ç”¨äºæ–°å»ºæ¨¡å‹æ—¶)
    MODEL_PARAMS = dict(
        num_atom_types=100, 
        hidden_dim=96, 
        num_layers=2, 
        cutoff=6.0, 
        num_rbf=10,
        use_L0=True, 
        use_L1=True,
        use_L2=True, 
        use_gating=True, 
        use_long_range=False,
    )

# ==========================================
# 2. è¾…åŠ©å‡½æ•°
# ==========================================
def init_distributed_mode():
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://", world_size=world_size, rank=rank)
        dist.barrier()
        return local_rank, rank, world_size
    else:
        print("âš ï¸ Warning: Running in Single GPU Mode")
        return 0, 0, 1

def log_info(msg, rank):
    if rank == 0:
        print(msg)

def get_dataloader(data_dir, meta_file, rank, world_size, is_train=True):
    full_path = os.path.join(data_dir, meta_file)
    if not os.path.exists(full_path):
        if is_train:
            raise FileNotFoundError(f"âŒ Error: {meta_file} not found!")
        else:
            log_info(f"âš ï¸ Warning: {meta_file} not found, skipping...", rank)
            return None, None

    dataset = ChunkedSmartDataset_h5(
        data_dir, metadata_file=meta_file, rank=rank, world_size=world_size
    )

    sampler = BinPackingSampler(
        dataset.metadata,
        max_cost=Config.MAX_COST_PER_BATCH,
        edge_weight="auto",
        shuffle=is_train,
        world_size=world_size,
        rank=rank
    )

    loader = DataLoader(
        dataset,
        batch_sampler=sampler,
        num_workers=Config.NUM_WORKERS,
        pin_memory=True,
        prefetch_factor=Config.PREFETCH_FACTOR,
    )
    return loader, sampler

def build_model(device, rank, model_config, state_dict=None):
    """
    ç»Ÿä¸€æ„å»ºé€»è¾‘ï¼š
    1. æ ¹æ® model_config å®ä¾‹åŒ–
    2. å¦‚æœæœ‰ state_dict åˆ™åŠ è½½æƒé‡
    3. å¦‚æœæ²¡æœ‰ state_dict åˆ™å°è¯•åŠ è½½ E0
    """
    model = HTGPModel(model_config).to(device)

    if state_dict is not None:
        # --- åŠ è½½æƒé‡ ---
        if rank == 0:
            log_info("ğŸ“¥ Loading state_dict from checkpoint...", rank)
        
        # å¤„ç† DDP çš„ 'module.' å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v 
            else:
                new_state_dict[k] = v
        
        # åŠ è½½æƒé‡ (Strict=False ä»¥é˜² E0 buffers ä¸åŒ¹é…ï¼Œè§†æƒ…å†µè€Œå®š)
        model.load_state_dict(new_state_dict, strict=False) 
    else:
        # --- æ–°è®­ç»ƒï¼šåŠ è½½ E0 ---
        if os.path.exists(Config.E0_PATH):
            meta_data = torch.load(Config.E0_PATH, map_location='cpu', weights_only=False)
            e0_dict = meta_data.get('e0_dict', None)
            model.load_external_e0(e0_dict)
            model.atomic_ref.weight.requires_grad = False
            if rank == 0:
                log_info(f"âœ¨ Injected E0 from {Config.E0_PATH}", rank)
        else:
            model.atomic_ref.weight = model.atomic_ref.weight.float()
            log_info("âš ï¸ E0 file not found, skipping injection.", rank)

    # æ‰“å°å‚æ•°é‡
    if rank == 0:
        param_count = sum(p.numel() for p in model.parameters())
        log_info(f"ğŸ§  Model Parameters: {param_count:,}", rank)

    # DDP åŒ…è£…
    if dist.is_initialized():
        model = DDP(model, device_ids=[device.index], output_device=device.index, find_unused_parameters=True)
    
    return model

# ==========================================
# 3. ä¸»ç¨‹åº (Main)
# ==========================================
def main():
    # --- A. åˆå§‹åŒ–ç¯å¢ƒ ---
    local_rank, rank, world_size = init_distributed_mode()
    device = torch.device(f"cuda:{local_rank}")
    
    if rank == 0:
        os.makedirs(Config.LOG_DIR, exist_ok=True)
        log_info(f"\nğŸš€ [Start] World Size: {world_size} | Device: {device}", rank)

    # --- B. å‡†å¤‡æ•°æ® ---
    log_info("\n[1/4] Initializing DataLoaders...", rank)
    train_loader, train_sampler = get_dataloader(Config.DATA_DIR, Config.TRAIN_META, rank, world_size, is_train=True)
    test_loader, test_sampler = get_dataloader(Config.DATA_DIR, Config.TEST_META, rank, world_size, is_train=False)

    # --- C. å‡†å¤‡é…ç½® (Restart é€»è¾‘çš„æ ¸å¿ƒ) ---
    # !!! è®¾ç½®è¿™é‡Œ !!!
    RESTART = True
    CHECKPOINT_PATH = "Checkpoints_break_2/model_epoch_47.pt"
    
    start_epoch = 0
    checkpoint_state = None
    
    # ä¼°ç®—æ€»æ­¥æ•° (Trainer éœ€è¦ç”¨)
    train_total_steps = train_sampler.precompute_total_steps(Config.EPOCHS)
    
    # é»˜è®¤æ¨¡å‹é…ç½®
    model_config = HTGPConfig(**Config.MODEL_PARAMS)

    if RESTART:
        log_info(f"\nğŸ”„ Resuming from {CHECKPOINT_PATH}...", rank)
        # åŠ è½½ Checkpoint
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)
        
        # 1. æ¢å¤ Epoch
        start_epoch = checkpoint.get('epoch', 47)
        
        # 2. æ¢å¤ Config (éå¸¸é‡è¦ï¼Œè¿™å°±åŒ…å«äº† avg_neighborhood)
        if 'model_config' in checkpoint:
            model_config = checkpoint['model_config']
            log_info(f"   Loaded config from checkpoint (avg_neigh={model_config.avg_neighborhood:.2f})", rank)
        else:
            # å¦‚æœä¹‹å‰çš„ checkpoint æ²¡å­˜ config (å¦‚ä½ ä¸Šä¸ªé—®é¢˜æ‰€è¯´)ï¼Œè¿™é‡Œåšä¸ªå…œåº•
            log_info("âš ï¸ No config in checkpoint, using default derived from data.", rank)
            model_config.avg_neighborhood = 1.0 / train_sampler.edge_weight
            
        # 3. è·å–æƒé‡å­—å…¸
        if 'model_state_dict' in checkpoint:
            checkpoint_state = checkpoint['model_state_dict']
        else:
            # å…¼å®¹åªä¿å­˜äº† state_dict çš„æƒ…å†µ
            checkpoint_state = checkpoint 
            
        # 4. è®¡ç®— Resume Step (ç”¨äº OneCycleLR)
        # å‡è®¾ sampler é€»è¾‘ä¸å˜ï¼Œä¼°ç®—ä¹‹å‰çš„æ€»æ­¥æ•°
        steps_per_epoch_est = train_total_steps // Config.EPOCHS
        resume_step = start_epoch * steps_per_epoch_est - 1
        log_info(f"   Resuming OneCycleLR from step: {resume_step}", rank)
        
    else:
        # æ–°è®­ç»ƒæ¨¡å¼
        log_info("\nğŸ†• Starting New Training...", rank)
        # è®¡ç®— avg_neighborhood
        model_config.avg_neighborhood = 1.0 / train_sampler.edge_weight
        resume_step = -1  # é»˜è®¤å€¼ï¼Œä»£è¡¨ä»å¤´å¼€å§‹

    # --- D. æ„å»ºæ¨¡å‹ ---
    log_info("\n[2/4] Building Model...", rank)
    # æ— è®ºæ˜¯å¦ restartï¼Œç»Ÿä¸€è°ƒç”¨ build_model
    model = build_model(device, rank, model_config, state_dict=checkpoint_state)

    # --- E. åˆå§‹åŒ– Trainer ---
    log_info("\n[3/4] Initializing Trainer...", rank)
    
    # æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿ä½ çš„ PotentialTrainer èƒ½å¤Ÿæ¥æ”¶ last_epoch å‚æ•°
    # å¹¶ä¼ é€’ç»™ optim.lr_scheduler.OneCycleLR(..., last_epoch=last_epoch)
    trainer = PotentialTrainer(
        model, 
        total_steps=train_total_steps,
        max_lr=Config.LR, 
        device=device, 
        checkpoint_dir=Config.LOG_DIR,
        last_epoch=resume_step  # <--- å°†è®¡ç®—å¥½çš„æ­¥æ•°ä¼ å…¥
    )

    if RESTART and checkpoint is not None:
        log_info("ğŸ”„ Restoring Optimizer, Scheduler and EMA states...", rank)
        trainer.load_checkpoint(checkpoint) # <--- è°ƒç”¨æ–°æ–¹æ³•

    # --- F. è®­ç»ƒå¾ªç¯ ---
    log_info(f"\n[4/4] Starting Loop (Epoch {start_epoch + 1} -> {Config.EPOCHS})...", rank)
    log_info("="*60, rank)

    # å¾ªç¯ä» start_epoch + 1 å¼€å§‹
    for epoch in range(start_epoch + 1, Config.EPOCHS + 1):
        train_sampler.set_epoch(epoch)
        
        # 1. Train
        train_metrics = trainer.train_epoch(train_loader, epoch_idx=epoch)
        
        # 2. Validate
        if test_loader:
            val_metrics = trainer.validate(test_loader, epoch_idx=epoch)
        else:
            val_metrics = {'total_loss': 0.0, 'mae_f': 0.0}

        # 3. Log & Save
        if rank == 0:
            log_msg = (
                f"Ep {epoch:03d} | "
                f"T_Loss: {train_metrics['total_loss']:.4f} | "
                f"V_Loss: {val_metrics['total_loss']:.4f} | "
                f"MAE_F: {train_metrics['mae_f']*1000:.1f}/{val_metrics['mae_f']*1000:.1f} meV/A"
            )
            print(log_msg)
            # ä¿å­˜ Checkpoint
            save_dict = {
                'epoch': epoch,
                'model_config': model_config,
                'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),
                
                # --- åŸæœ‰éƒ¨åˆ† ---
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'scheduler_state_dict': trainer.scheduler.state_dict(),
                
                # ğŸ”¥ æ–°å¢ï¼šåŠ¡å¿…ä¿å­˜ EMA çŠ¶æ€ï¼
                'ema_state_dict': trainer.ema.state_dict(), 
            }
            torch.save(save_dict, os.path.join(Config.LOG_DIR, f'model_epoch_{epoch}.pt'))

        if dist.is_initialized():
            dist.barrier()

    log_info("\nâœ… Training Finished!", rank)
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    os.environ["OMP_NUM_THREADS"] = "1" 
    main()
