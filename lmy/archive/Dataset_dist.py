import os
import glob
import random
import torch
import math
import torch.distributed as dist
from torch.utils.data import IterableDataset

class ShardedPyGDataset(IterableDataset):
    def __init__(self, data_dir, prefix, shuffle=False):
        """
        :param data_dir: æ•°æ®ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        :param prefix: æ–‡ä»¶å‰ç¼€ (e.g., "train" æˆ– "test")
        :param shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ® (è®­ç»ƒé›† True, æµ‹è¯•é›† False)
        """
        super().__init__()
        self.data_dir = data_dir
        pattern = os.path.join(data_dir, f"{prefix}_*.pt")
        self.file_paths = sorted(glob.glob(pattern))
        self.shuffle = shuffle
        
        if len(self.file_paths) == 0:
            raise FileNotFoundError(f"âŒ æœªåœ¨ {data_dir} æ‰¾åˆ°ä»¥ {prefix} å¼€å¤´çš„æ–‡ä»¶ï¼")
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if not dist.is_initialized() or dist.get_rank() == 0:
            print(f"ğŸ“‚ [Dataset] Found {len(self.file_paths)} parts for '{prefix}'")

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        files = self.file_paths.copy()

        # ============================================================
        # 1. DDP åˆ‡åˆ†: æŒ‰ GPU Rank åˆ†é…æ–‡ä»¶
        # ============================================================
        if dist.is_initialized():
            rank = dist.get_rank()
            world_size = dist.get_world_size()
            # é—´éš”é‡‡æ ·: GPU0->[0,4,8], GPU1->[1,5,9]...
            files = files[rank::world_size]

        # ============================================================
        # 2. Worker åˆ‡åˆ†: æŒ‰ CPU è¿›ç¨‹åˆ†é…æ–‡ä»¶
        # ============================================================
        if worker_info is not None:
            per_worker = int(math.ceil(len(files) / float(worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = worker_id * per_worker
            iter_end = min(iter_start + per_worker, len(files))
            files = files[iter_start:iter_end]

        if self.shuffle:
            random.shuffle(files)

        for file_path in files:
            try:
                # è¯»å–æ•°æ® (å¯èƒ½æ˜¯ int8/int32/float32 æ··åˆ)
                chunk_data = torch.load(file_path, weights_only=False)
                
                if self.shuffle:
                    random.shuffle(chunk_data)
                
                for data in chunk_data:
                    # ========================================================
                    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒï¼šå¼ºåˆ¶ç±»å‹ä¿®æ­£ (Float32 / Int64) ğŸ”¥ğŸ”¥ğŸ”¥
                    # ========================================================
                    
                    # --- A. ç´¢å¼•ç±»å¿…é¡»æ˜¯ Int64 (Long) ---
                    if hasattr(data, 'edge_index') and data.edge_index is not None:
                        data.edge_index = data.edge_index.to(torch.long)
                    
                    if hasattr(data, 'z') and data.z is not None:
                        data.z = data.z.to(torch.long)
                        
                    if hasattr(data, 'edge_type') and data.edge_type is not None:
                        data.edge_type = data.edge_type.to(torch.long)

                    # --- B. æ•°å€¼ç±»å¿…é¡»æ˜¯ Float32 ---
                    # 1. åæ ‡
                    data.pos = data.pos.to(torch.float32)
                    
                    # 2. æ™¶èƒ
                    if hasattr(data, 'cell') and data.cell is not None:
                        data.cell = data.cell.to(torch.float32)
                    
                    # 3. å‘¨æœŸæ€§ä½ç§» (å¤„ç† int8 å‹ç¼©)
                    if hasattr(data, 'shifts_int'):
                        data.shifts = data.shifts_int.to(torch.float32)
                        del data.shifts_int # åˆ é™¤æ—§å±æ€§
                    elif hasattr(data, 'shifts') and data.shifts is not None:
                        data.shifts = data.shifts.to(torch.float32)

                    # 4. æ ‡ç­¾ (èƒ½é‡/åŠ›/åº”åŠ›)
                    if hasattr(data, 'y') and data.y is not None:
                        data.y = data.y.to(torch.float32)
                        
                    if hasattr(data, 'force') and data.force is not None:
                        data.force = data.force.to(torch.float32)
                        
                    if hasattr(data, 'stress') and data.stress is not None:
                        data.stress = data.stress.to(torch.float32)

                    yield data
                    
            except Exception as e:
                if not dist.is_initialized() or dist.get_rank() == 0:
                    print(f"âš ï¸ Error loading {file_path}: {e}")
                continue
