# ==========================================

# ğŸ”¥ å¿…é¡»æ”¾åœ¨æ–‡ä»¶æœ€æœ€æœ€å¼€å¤´ï¼ğŸ”¥

# åœ¨å¯¼å…¥ä»»ä½• torch/numpy ä¹‹å‰å°±é™åˆ¶çº¿ç¨‹

# ==========================================

import os

os.environ["OMP_NUM_THREADS"] = "1"

os.environ["MKL_NUM_THREADS"] = "1"

os.environ["NUMEXPR_NUM_THREADS"] = "1"

os.environ["VECLIB_MAXIMUM_THREADS"] = "1"

os.environ["OPENBLAS_NUM_THREADS"] = "1"

# ==========================================

# ç°åœ¨æ‰å¼€å§‹å¯¼å…¥å…¶ä»–åº“

# ==========================================

import torch

import random

import multiprocessing

import gc

import numpy as np

from tqdm.auto import tqdm

# ç§»é™¤ E0 ç›¸å…³çš„ import

from extxyz_to_pyg_custom import extxyz_to_pyg_custom


# ==========================================

# Worker ä»»åŠ¡

# ==========================================

def worker_task(args):
    # åŒé‡ä¿é™©ï¼šåœ¨è¿›ç¨‹å†…å†æ¬¡å¼ºåˆ¶è®¾ç½® PyTorch çº¿ç¨‹

    torch.set_num_threads(1)

    # ç§»é™¤ need_e0_sample å‚æ•°

    (worker_id, file_paths, save_dir, prefix, cutoff, chunk_size) = args

    buffer = []

    save_counter = 0

    # ç§»é™¤ e0_samples åˆ—è¡¨

    try:

        for fpath in file_paths:

            if os.path.getsize(fpath) == 0: continue

            try:

                data_list = extxyz_to_pyg_custom(fpath, cutoff=cutoff)

            except Exception:

                continue

            if not data_list: continue

            for data in data_list:

                buffer.append(data)

                # [å·²ç§»é™¤] E0 é‡‡æ ·é€»è¾‘

                # å­˜ç›˜é€»è¾‘

                if len(buffer) >= chunk_size:
                    save_name = f"{prefix}_w{worker_id}_part_{save_counter}.pt"

                    torch.save(buffer, os.path.join(save_dir, save_name))

                    buffer = []

                    save_counter += 1

                    gc.collect()  # é‡Šæ”¾å†…å­˜

        # å¤„ç†å‰©ä½™æ•°æ®

        if len(buffer) > 0:
            save_name = f"{prefix}_w{worker_id}_part_{save_counter}.pt"

            torch.save(buffer, os.path.join(save_dir, save_name))

            buffer = []

            gc.collect()

        return True  # ä¸å†è¿”å› E0 æ ·æœ¬ï¼Œä»…è¿”å›å®Œæˆæ ‡å¿—



    except Exception as e:

        print(f"âŒ Worker-{worker_id} Error: {e}")

        return False


# ==========================================

# ç®¡ç†å™¨

# ==========================================

def process_manager(file_files, save_dir, prefix, num_workers, chunk_size, cutoff):
    # ç§»é™¤ calc_e0 å‚æ•°

    if not os.path.exists(save_dir): os.makedirs(save_dir)

    # åŠ¨æ€è°ƒæ•´ worker æ•°é‡

    real_workers = min(num_workers, len(file_files))

    if real_workers == 0: return

    chunked_files = np.array_split(file_files, real_workers)

    tasks = []

    for i in range(real_workers):
        # ç§»é™¤ä»»åŠ¡å‚æ•°ä¸­çš„ calc_e0

        tasks.append((i, chunked_files[i].tolist(), save_dir, prefix, cutoff, chunk_size))

    print(f"ğŸš€ [Start] {prefix}: {len(file_files)} files -> {real_workers} Workers")

    # ä¸å†æ”¶é›† E0 ç»“æœï¼Œåªæ˜¯å•çº¯è·‘å®Œè¿›åº¦æ¡

    with multiprocessing.Pool(processes=real_workers) as pool:

        for _ in tqdm(pool.imap_unordered(worker_task, tasks), total=real_workers):
            pass

    return


# ==========================================

# ä¸»ç¨‹åº

# ==========================================

def main():
    # 1. å°è¯•ä¿®æ”¹å…±äº«ç­–ç•¥

    try:

        torch.multiprocessing.set_sharing_strategy('file_system')

    except:
        pass

    # 2. å‡†å¤‡æ–‡ä»¶

    file_dirs = [r"005_all", r"100_all", r"outcar_selected_xyz", r"xyz_grouped"]

    all_files = []

    # ä¿®æ­£äº†ä½ åŸä»£ç ä¸­ unqie_names é›†åˆç”Ÿæˆçš„é€»è¾‘é”™è¯¯

    unique_names = set()

    for d in file_dirs:

        if os.path.exists(d):

            # è·å–å®Œæ•´è·¯å¾„

            files_in_dir = [os.path.join(d, f) for f in os.listdir(d) if f.endswith('.xyz')]
            all_files.extend(files_in_dir)

            # è·å–å”¯ä¸€æ ‡è¯†å

            for f in os.listdir(d):

                if f.endswith('.xyz'):
                    unique_names.add(f.split('.')[0])
    print(f"ğŸ“‚ Found {len(all_files)} files with {len(unique_names)} unique names.")

    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒå‚æ•°å»ºè®® ğŸ”¥ğŸ”¥ğŸ”¥

    NUM_WORKERS = 96  # å»ºè®® 8-16ï¼ŒIOç“¶é¢ˆä¸‹ 64 å¯èƒ½ä¼šå¡é¡¿

    TRAIN_CHUNK_SIZE = 5120

    test_ratio = 0.05

    SAVE_DIR = "processed_dataset"

    CUTOFF = 6.0

    # åˆ’åˆ†æ•°æ®é›† (åŸºäº unique name)

    unique_names_list = sorted(list(unique_names))  # è½¬ä¸ºåˆ—è¡¨å¹¶æ’åº

    random.seed(42)

    random.shuffle(unique_names_list)

    num_test = max(1, int(len(unique_names_list) * test_ratio))

    test_names_set = set(unique_names_list[:num_test])

    train_names_set = set(unique_names_list[num_test:])

    # é‡æ–°è¿‡æ»¤æ–‡ä»¶

    train_files = []

    test_files = []

    for f in all_files:

        fname = f.split(os.sep)[-1].split('.')[0]

        if fname in train_names_set:

            train_files.append(f)

        elif fname in test_names_set:

            test_files.append(f)

    # ç®€å•æ‰“ä¹±æ–‡ä»¶é¡ºåº

    random.shuffle(train_files)

    random.shuffle(test_files)

    print(f"ğŸ“ Tasks: Train={len(train_files)}, Test={len(test_files)}")

    print(f"âš™ï¸ Config: Workers={NUM_WORKERS}, Chunk={TRAIN_CHUNK_SIZE}")

    # æ‰§è¡Œå¤„ç† (ç§»é™¤äº† calc_e0 å‚æ•°å’Œè¿”å›å€¼æ¥æ”¶)

    print("\n--- Processing Test Set ---")

    process_manager(test_files, SAVE_DIR, "test", NUM_WORKERS, 5120, CUTOFF)

    print("\n--- Processing Train Set ---")

    process_manager(train_files, SAVE_DIR, "train", NUM_WORKERS, TRAIN_CHUNK_SIZE, CUTOFF)

    # [å·²ç§»é™¤] E0 è®¡ç®—å’Œ meta_data.pt ä¿å­˜éƒ¨åˆ†

    print("\nâœ… All processing finished.")


if __name__ == '__main__':
    multiprocessing.freeze_support()

    main()
