# ==========================================
# ğŸ”¥ å¿…é¡»æ”¾åœ¨æ–‡ä»¶æœ€æœ€æœ€å¼€å¤´ï¼ğŸ”¥
# åœ¨å¯¼å…¥ä»»ä½• torch/numpy ä¹‹å‰å°±é™åˆ¶çº¿ç¨‹
# ==========================================
import os
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"

# ==========================================
# ç°åœ¨æ‰å¼€å§‹å¯¼å…¥å…¶ä»–åº“
# ==========================================
import torch
import random
import multiprocessing
import gc
import numpy as np
from tqdm.auto import tqdm

# ç¡®ä¿ä½ çš„è¾…åŠ©å‡½æ•°åœ¨åŒçº§ç›®å½•ä¸‹
from compute_average_e0 import compute_average_e0
from extxyz_to_pyg_custom import extxyz_to_pyg_custom

# ==========================================
# Worker ä»»åŠ¡
# ==========================================
def worker_task(args):
    # åŒé‡ä¿é™©ï¼šåœ¨è¿›ç¨‹å†…å†æ¬¡å¼ºåˆ¶è®¾ç½® PyTorch çº¿ç¨‹
    torch.set_num_threads(1)
    
    (worker_id, file_paths, save_dir, prefix, cutoff, chunk_size, need_e0_sample) = args
    
    buffer = []
    save_counter = 0
    e0_samples = []
    
    try:
        # è°ƒè¯•æ‰“å°ï¼šç¡®è®¤è¯¥ Worker å¯åŠ¨
        # print(f"ğŸ”§ Worker-{worker_id} å¯åŠ¨ï¼Œå¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        for fpath in file_paths:
            if os.path.getsize(fpath) == 0: continue
            
            try:
                data_list = extxyz_to_pyg_custom(fpath, cutoff=cutoff)
            except Exception:
                continue
            
            if not data_list: continue
            
            for data in data_list:
                buffer.append(data)
                
                # æ”¶é›†å°‘é‡ E0 æ ·æœ¬
                if need_e0_sample and len(e0_samples) < 3000:
                    e0_samples.append(data)

                # å­˜ç›˜é€»è¾‘
                if len(buffer) >= chunk_size:
                    save_name = f"{prefix}_w{worker_id}_part_{save_counter}.pt"
                    torch.save(buffer, os.path.join(save_dir, save_name))
                    buffer = [] 
                    save_counter += 1
                    gc.collect() # é‡Šæ”¾å†…å­˜
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if len(buffer) > 0:
            save_name = f"{prefix}_w{worker_id}_part_{save_counter}.pt"
            torch.save(buffer, os.path.join(save_dir, save_name))
            buffer = []
            gc.collect()
            
        return e0_samples
        
    except Exception as e:
        print(f"âŒ Worker-{worker_id} Error: {e}")
        return []

# ==========================================
# ç®¡ç†å™¨
# ==========================================
def process_manager(file_files, save_dir, prefix, num_workers, chunk_size, cutoff, calc_e0):
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    
    # åŠ¨æ€è°ƒæ•´ worker æ•°é‡
    real_workers = min(num_workers, len(file_files))
    if real_workers == 0: return []
    
    chunked_files = np.array_split(file_files, real_workers)
    
    tasks = []
    for i in range(real_workers):
        tasks.append((i, chunked_files[i].tolist(), save_dir, prefix, cutoff, chunk_size, calc_e0))
    
    print(f"ğŸš€ [Start] {prefix}: {len(file_files)} files -> {real_workers} Workers")
    
    collected_e0 = []
    
    # ä½¿ç”¨ spawn å¯åŠ¨æ–¹å¼å¯ä»¥æ›´å½»åº•åœ°éš”ç¦»ç¯å¢ƒï¼ˆå¯é€‰ï¼Œä½†é€šå¸¸ fork å°±å¤Ÿäº†å¦‚æœ env è®¾ç½®å¾—æ—©ï¼‰
    # ctx = multiprocessing.get_context('spawn')
    # with ctx.Pool(processes=real_workers) as pool:
    
    with multiprocessing.Pool(processes=real_workers) as pool:
        for res in tqdm(pool.imap_unordered(worker_task, tasks), total=real_workers):
            collected_e0.extend(res)
            
    return collected_e0

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    # 1. å°è¯•ä¿®æ”¹å…±äº«ç­–ç•¥
    try:
        torch.multiprocessing.set_sharing_strategy('file_system')
    except: pass

    # 2. å‡†å¤‡æ–‡ä»¶
    file_dirs = [r"005", r"100", r"outcar_selected_xyz", r"xyz_grouped"]
    all_files = []
    unqie_names = set()
    for d in file_dirs:
        if os.path.exists(d):
            all_files.extend([os.path.join(d, f) for f in os.listdir(d) if f.endswith('.xyz')]) # å®Œæ•´è·¯å¾„
            unqie_names.add(f.split('.')[0] for f in os.listdir(d) if f.endswith('.xyz'))
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒå‚æ•°å»ºè®® ğŸ”¥ğŸ”¥ğŸ”¥
    # å³ä½¿ä½ æœ‰ 60 æ ¸ï¼Œä¹Ÿä¸è¦è¶…è¿‡ 16ã€‚IO ç“¶é¢ˆä¸‹ï¼Œæ ¸å¤šåè€Œæ…¢ã€‚
    NUM_WORKERS = 64      # å»ºè®® 8-12ï¼Œç»å¯¹ä¸è¦ 60
    TRAIN_CHUNK_SIZE = 5120
    test_ratio = 0.05
    SAVE_DIR = "processed_dataset"
    CUTOFF = 6.0
    
    # åˆ’åˆ†
    # æŒ‰ç…§unique nameåˆ’åˆ†ï¼Œç¡®ä¿åŒä¸€ææ–™ä¸åœ¨trainå’Œtesté‡Œ
    random.seed(42)
    all_files_sorted = sorted(all_files, key=lambda x: x.split(os.sep)[-1].split('.')[0]) # æŒ‰åç§°æ’åºï¼Œç¡®ä¿åŒä¸€ææ–™æ–‡ä»¶åœ¨ä¸€èµ·
    unique_names = sorted(list(unqie_names)) # æ’åºä»¥ç¡®ä¿å¯å¤ç°
    random.shuffle(unique_names)
    num_test = max(1, int(len(unique_names) * test_ratio)) # è‡³å°‘1ä¸ª
    test_names = set(unique_names[:num_test]) # æµ‹è¯•é›†åç§°
    train_names = set(unique_names[num_test:]) # è®­ç»ƒé›†åç§°
    # å®Œæ•´è·¯å¾„åˆ†é…
    train_files = [f for f in all_files_sorted if f.split(os.sep)[-1].split('.')[0] in train_names]
    test_files = [f for f in all_files_sorted if f.split(os.sep)[-1].split('.')[0] in test_names]
    manual = False
    if manual:
        train_files = train_files[:train_end]
        test_files = test_files[train_end:test_end]
    else:
        train_files = train_files
        test_files = test_files
        
    print(f"ğŸ“ Tasks: Train={len(train_files)}, Test={len(test_files)}")
    print(f"âš™ï¸ Config: Workers={NUM_WORKERS}, Chunk={TRAIN_CHUNK_SIZE}")

    # æ‰§è¡Œ
    process_manager(test_files, SAVE_DIR, "test", NUM_WORKERS, 5120, CUTOFF, False)
    train_e0 = process_manager(train_files, SAVE_DIR, "train", NUM_WORKERS, TRAIN_CHUNK_SIZE, CUTOFF, True)
#    process_manager(test_files, SAVE_DIR, "test", NUM_WORKERS, 2000, CUTOFF, False)

    # ä¿å­˜ Meta
    if train_e0:
        print("Calculating E0...")
        e0_dict = compute_average_e0(train_e0[:3000])
        torch.save({'e0_dict': e0_dict}, os.path.join(SAVE_DIR, "meta_data.pt"))
        print("âœ… Done.")

if __name__ == '__main__':
    # åªæœ‰åœ¨ main é‡Œæ‰ freeze
    multiprocessing.freeze_support()
    main()
