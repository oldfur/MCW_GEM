import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from ase.io import read
from Utils import scatter_add
from ase.neighborlist import neighbor_list
from torch_geometric.data import Data
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass, field
from Utils import HTGPConfig
# ==========================================

# class BesselBasis(nn.Module): # è´å¡å°”åŸºå‡½æ•°æ„å»ºæ¨¡å—
#     def __init__(self, r_max, num_basis=8):
#         super().__init__()
#         self.r_max = float(r_max)
#         self.num_basis = num_basis
#         # é¢„è®¡ç®—é¢‘ç‡ (buffer ä¼šè‡ªåŠ¨éš model.to("cuda") ç§»åŠ¨)
#         self.register_buffer("freq", torch.arange(1, num_basis + 1).float() * np.pi)

#     def forward(self, d):
#         # d: [Edges, 1] (åœ¨ GPU ä¸Š)
#         d_scaled = d / self.r_max
#         # print(d.device)
#         prefactor = torch.sqrt(torch.tensor(2.0 / self.r_max, device=d.device))
#         # print(prefactor.device)
#         # print(self.freq.device)
#         # float * GPU_Tensor -> è‡ªåŠ¨æ­£å¸¸è¿è¡Œ
#         return prefactor * torch.sin(self.freq * d_scaled) / (d + 1e-6)

# 1. Bessel Math (JIT Engine)
@torch.jit.script
def compute_bessel_math(d: torch.Tensor, r_max: float, freq: torch.Tensor) -> torch.Tensor:
    """Bessel Basis çš„çº¯æ•°å­¦è®¡ç®—éƒ¨åˆ†"""
    d_scaled = d / r_max
    prefactor = (2.0 / r_max) ** 0.5
    return prefactor * torch.sin(freq * d_scaled) / (d + 1e-6)

# ==========================================
# ğŸš€ è¿˜åŸä¸ºæ™®é€šçš„ nn.Module (è°ƒç”¨ä¸Šé¢çš„ JIT å‡½æ•°)
# ==========================================

class BesselBasis(nn.Module): 
    # âŒ åˆ æ‰äº† @torch.jit.scriptï¼Œå˜å›æ™®é€šç±»
    def __init__(self, r_max: float, num_basis: int = 8):
        super().__init__()
        self.r_max = float(r_max)
        self.num_basis = int(num_basis)
        self.register_buffer("freq", torch.arange(1, num_basis + 1).float() * np.pi)

    def forward(self, d: torch.Tensor) -> torch.Tensor:
        # âœ… è°ƒç”¨ JIT å‡½æ•°ï¼Œäº«å—åŠ é€Ÿ
        return compute_bessel_math(d, self.r_max, self.freq)


# # [NEW] åŒ…ç»œå‡½æ•°ï¼šä¿è¯ cutoff å¤„èƒ½é‡å’ŒåŠ›å¹³æ»‘è¡°å‡ä¸º 0
# class PolynomialEnvelope(nn.Module):
#     def __init__(self, r_cut, p=5):
#         super().__init__()
#         self.r_cutoff = r_cut
#         self.p = p # è¿™é‡Œpå…¶å®æ²¡ç”¨åˆ°ï¼Œå…¬å¼æ˜¯å›ºå®šçš„ï¼Œä¸ºäº†å…¼å®¹æ€§ä¿ç•™å³å¯
    
    
#     def forward(self, d_ij):
#         # 1. å½’ä¸€åŒ–è·ç¦» x = d / r_cut
#         # èŒƒå›´ä» [0, r_cut] æ˜ å°„åˆ° [0, 1]
#         x = d_ij / self.r_cutoff
        
#         # 2. æˆªæ–­ä¿æŠ¤
#         # è™½ç„¶é‚»å±…åˆ—è¡¨é€šå¸¸åªåŒ…å« < r_cut çš„åŸå­ï¼Œä½†ä¸ºäº†æ•°å€¼å®‰å…¨ï¼Œ
#         # å¿…é¡»ç¡®ä¿ x ä¸è¶…è¿‡ 1ï¼Œå¦åˆ™å¤šé¡¹å¼ä¼šå‘æ•£ã€‚
#         # å®é™…ä¸Šï¼Œå¯¹äº x > 1 çš„éƒ¨åˆ†ï¼ŒåŒ…ç»œå€¼åº”è¯¥ä¸¥æ ¼ä¸º 0ã€‚
#         # è¿™é‡Œçš„ clamp ä¿è¯ x åœç•™åœ¨ 1ï¼Œä»£å…¥å…¬å¼ç»“æœä¸º 0ã€‚
#         x = torch.clamp(x, min=0, max=1)
        
#         # 3. è®¡ç®—å¤šé¡¹å¼
#         # 1 - 10x^3 + 15x^4 - 6x^5
#         return 1 - 10 * x**3 + 15 * x**4 - 6 * x**

# 2. Envelope Math (JIT Engine)
@torch.jit.script
def compute_envelope_math(d: torch.Tensor, r_cut: float) -> torch.Tensor:
    """Envelope çš„çº¯æ•°å­¦è®¡ç®—éƒ¨åˆ†"""
    x = d / r_cut
    x = torch.clamp(x, min=0.0, max=1.0)
    return 1.0 - 10.0 * x**3 + 15.0 * x**4 - 6.0 * x**5
# ==========================================
class PolynomialEnvelope(nn.Module):
    # âŒ åˆ æ‰äº† @torch.jit.script
    def __init__(self, r_cut: float, p: int = 5):
        super().__init__()
        self.r_cutoff = float(r_cut)
        self.p = int(p)
    
    def forward(self, d_ij: torch.Tensor) -> torch.Tensor:
        # âœ… è°ƒç”¨ JIT å‡½æ•°
        return compute_envelope_math(d_ij, self.r_cutoff)


@torch.jit.script
def compute_l2_basis(rbf_feat: torch.Tensor, r_hat: torch.Tensor) -> torch.Tensor:
    # rbf_feat: (E, F)
    # r_hat: (E, 3)
    
    # å¤–ç§¯: (E, 3, 1) * (E, 1, 3) -> (E, 3, 3)
    outer = r_hat.unsqueeze(2) * r_hat.unsqueeze(1) 
    
    # æ„é€ å•ä½é˜µï¼Œæ³¨æ„ä½¿ç”¨ type_as ä¿æŒè®¾å¤‡å’Œç±»å‹ä¸€è‡´
    eye = torch.eye(3, dtype=r_hat.dtype, device=r_hat.device).unsqueeze(0)
    
    # å»è¿¹
    trace_less = outer - (1.0/3.0) * eye
    
    # èåˆ: (E, 1, 1, F) * (E, 3, 3, 1) -> (E, 3, 3, F)
    return rbf_feat.unsqueeze(1).unsqueeze(1) * trace_less.unsqueeze(-1)

class GeometricBasis(nn.Module): # å‡ ä½•åŸºåº•æ„å»ºæ¨¡å—
    """
    æ„å»ºæ­£äº¤ç¬›å¡å°”-å„ç±³ç‰¹åŸºåº• T^(L) å¹¶ä¸å¾„å‘ç‰¹å¾ R(d) èåˆ
    """
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.rbf = BesselBasis(config.cutoff, config.num_rbf)
        self.envelope = PolynomialEnvelope(r_cut=config.cutoff)
        self.rbf_mlp = nn.Sequential(
            nn.Linear(config.num_rbf, config.hidden_dim),
            nn.SiLU(), # æ¿€æ´»å‡½æ•° x / (1 + exp(-x))
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )

    def forward(self, vec_ij, d_ij):
        # vec_ij: (E, 3), d_ij: (E,)
        raw_rbf = self.rbf_mlp(self.rbf(d_ij.unsqueeze(-1))) # (E, F)
        env = self.envelope(d_ij) # åŒ…ç»œ
        rbf_feat = raw_rbf * env.unsqueeze(-1)  # (E, F)

        r_hat = vec_ij / (d_ij.unsqueeze(-1) + 1e-6) # (E, 3)
        
        basis = {}
        
        # L=0: Scalar [R(d)]
        basis[0] = rbf_feat
        
        # L=1: Vector [R(d) * r_hat]
        if self.cfg.use_L1 or self.cfg.use_L2:
            basis[1] = rbf_feat.unsqueeze(1) * r_hat.unsqueeze(-1) # (E, 3, F)
            
        # L=2: Tensor [R(d) * (r_hat x r_hat - I/3)]
        if self.cfg.use_L2:
            # outer = torch.bmm(r_hat.unsqueeze(2), r_hat.unsqueeze(1)) # (E, 3, 3)
            # eye = torch.eye(3, device=vec_ij.device).unsqueeze(0)
            # trace_less = outer - (1.0/3.0) * eye
            # basis[2] = rbf_feat.unsqueeze(1).unsqueeze(1) * trace_less.unsqueeze(-1) # (E, 3, 3, F)
            basis[2] = compute_l2_basis(rbf_feat, r_hat) # (E, 3, 3, F)            
        return basis, r_hat

# ==========================================
# 3. åŠ¨åŠ›å­¦å¼•æ“: è±å¸ƒå°¼èŒ¨è€¦åˆ (Leibniz Coupling)
# ==========================================
class LeibnizCoupling(nn.Module): # æ¶ˆæ¯ä¼ é€’æ¨¡å—
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        self.path_weights = nn.ModuleDict()
        
        # åŠ¨æ€æ³¨å†Œéœ€è¦çš„çº¿æ€§å±‚
        for path_key, active in config.active_paths.items():
            if not active: continue
            # æ£€æŸ¥ L2 å¼€å…³
            l_in, l_edge, l_out, _ = path_key
            if (l_in == 2 or l_edge == 2 or l_out == 2) and not config.use_L2:
                continue
            if (l_in == 1 or l_edge == 1 or l_out == 1) and not config.use_L1:
                continue
                
            name = f"{l_in}_{l_edge}_{l_out}_{path_key[3]}"
            self.path_weights[name] = nn.Linear(self.F, self.F, bias=False)

        # [ä¿®æ”¹ç‚¹ 1] å½’ä¸€åŒ–å¸¸æ•°ï¼Œé˜²æ­¢æ·±å±‚æ•°å€¼çˆ†ç‚¸
        self.inv_sqrt_f = self.F ** -0.5

    def forward(self, h_nodes, basis_edges, edge_index):
        src, _ = edge_index
        messages = {0: [], 1: [], 2: []}
        
        for path_key, active in self.cfg.active_paths.items():
            if not active: continue
            
            l_in, l_edge, l_out, op_type = path_key
            
            # å®‰å…¨æ€§æ£€æŸ¥: å¦‚æœè¯¥å±‚è¾“å…¥ç‰¹å¾ä¸å­˜åœ¨ (å¦‚ç¬¬ä¸€å±‚æ²¡æœ‰ h1 h2)
            if basis_edges.get(l_edge) is None: continue
            
            # 1. è·å–æƒé‡å±‚ & çº¿æ€§å˜æ¢
            layer_name = f"{l_in}_{l_edge}_{l_out}_{op_type}"
            if layer_name not in self.path_weights: continue # è¢« Config å¼€å…³å…¨å±€ç¦ç”¨
            
            if h_nodes.get(l_in) is None:
                # æ„é€ å…¨é›¶å¼ é‡ï¼Œç»´åº¦æ ¹æ® l_in å†³å®š
                # l=0: (N, F), l=1: (N, 3, F), l=2: (N, 3, 3, F)
                num_nodes = h_nodes[0].size(0) # ä»¥ h0 ä¸ºåŸºå‡†è·å–èŠ‚ç‚¹æ•°
                shape = (num_nodes,) + ((3,) * l_in) + (self.F,)
                inp = torch.zeros(shape, device=h_nodes[0].device, dtype=h_nodes[0].dtype)
            else:
                inp = h_nodes[l_in]
            
            h_src = inp[src]
            h_trans = self.path_weights[layer_name](h_src) # Linear Transform
            geom = basis_edges[l_edge]
            
            res = None
            
            # === è¿ç®—æ ¸å¿ƒé€»è¾‘ (Operation Kernels) ===
            
            # 1. ç®€å•ç§¯ (Prod): Scalar scaling
            if op_type == 'prod':              
                # --- Case A: æ ‡é‡ * æ ‡é‡ -> æ ‡é‡ ---
                if l_in == 0 and l_edge == 0: 
                    # (E, F) * (E, F) -> (E, F)
                    res = h_trans * geom
                
                # --- Case B: æ ‡é‡ é©±åŠ¨ å‡ ä½• ---
                # (0, 1, 1): s * v -> v (ç”Ÿæˆåˆå§‹å¶æ)
                elif l_in == 0 and l_edge == 1: 
                    # (E, F) * (E, 3, F) -> (E, 3, F)
                    # éœ€è¦ unsqueeze h_trans
                    res = h_trans.unsqueeze(1) * geom
                
                # (0, 2, 2): s * t -> t (ç”Ÿæˆåˆå§‹å››æ) [ä½ æ–°å¢çš„]
                elif l_in == 0 and l_edge == 2:
                    # (E, F) * (E, 3, 3, F) -> (E, 3, 3, F)
                    res = h_trans.unsqueeze(1).unsqueeze(1) * geom

                # --- Case C: å‡ ä½• é©±åŠ¨ æ ‡é‡ (å¾„å‘ç¼©æ”¾) ---
                # (1, 0, 1): v * s -> v (å¾„å‘ç¼©æ”¾) [ä½ æ–°å¢çš„]
                elif l_in == 1 and l_edge == 0:
                    # (E, 3, F) * (E, F) -> (E, 3, F)
                    # éœ€è¦ unsqueeze geom
                    res = h_trans * geom.unsqueeze(1)
                
                # (2, 0, 2): t * s -> t (å¾„å‘ç¼©æ”¾)
                elif l_in == 2 and l_edge == 0:
                    # (E, 3, 3, F) * (E, F) -> (E, 3, 3, F)
                    res = h_trans * geom.unsqueeze(1).unsqueeze(1)

            # 2. ç‚¹ç§¯ (Dot): Contraction -> L_out < L_in
            elif op_type == 'dot': # v . v -> s
                res = torch.sum(h_trans * geom, dim=1)

            # 3. å‰ç§¯ (Cross): Vector Cross Product -> L_out = L_in = 1
            elif op_type == 'cross': # v x v -> v (Chiral Interaction)
                g = geom
                if g.dim() == 2: # å¦‚æœ geom æ˜¯ (E, 3)
                     g = g.unsqueeze(-1) # (E, 3, 1) - å¹¿æ’­
                
                res = torch.linalg.cross(h_trans, g, dim=1)   

            # 4. å¤–ç§¯ (Outer): Product -> L_out > L_in
            elif op_type == 'outer': # v x v -> t (Traceless)
                outer = h_trans.unsqueeze(2) * geom.unsqueeze(1) # (E,3,1,F)*(E,1,3,F) -> (E,3,3,F)
                # å»è¿¹ (Remove Trace)
                trace = torch.einsum('eiif->ef', outer)
                eye = torch.eye(3, device=outer.device).view(1, 3, 3, 1)
                res = outer - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye

            # 5. çŸ©é˜µ-çŸ¢é‡ (Mat-Vec / Vec-Mat): L=2 & L=1 -> L=1
            elif op_type == 'mat_vec': # T . v -> v
                res = torch.einsum('eijf, ejf -> eif', h_trans, geom)
            elif op_type == 'vec_mat': # v . T -> v
                res = torch.einsum('eif, eijf -> ejf', h_trans, geom)
                
            # 6. åŒç‚¹ç§¯ (Double Dot): L=2 : L=2 -> L=0
            elif op_type == 'double_dot': 
                res = torch.sum(h_trans * geom, dim=(1, 2))

            # 7. å¯¹ç§°çŸ©é˜µä¹˜ (Mat Mul Sym): L=2 x L=2 -> L=2
            elif op_type == 'mat_mul_sym':
                # Matrix Mul
                raw = torch.einsum('eikf, ekjf -> eijf', h_trans, geom)
                # Symmetrize
                sym = 0.5 * (raw + raw.transpose(1, 2))
                # Trace removal
                trace = torch.einsum('eiif->ef', sym)
                eye = torch.eye(3, device=sym.device).view(1, 3, 3, 1)
                res = sym - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye

            if res is not None:
                messages[l_out].append(res * self.inv_sqrt_f) # åº”ç”¨å½’ä¸€åŒ–å¸¸æ•°
                
        # ç»“æœèšåˆ (Simple Summation)
        final_msgs = {}
        for l in [0, 1, 2]:
            final_msgs[l] = sum(messages[l]) if messages[l] else None
        return final_msgs


# ğŸ”¥ [JIT å‡½æ•°] è®¡ç®—ç‰©ç†æŠ•å½± (fuse sum + mul + cat)
@torch.jit.script
def compute_gating_projections(h_node1: torch.Tensor, 
                               r_hat: torch.Tensor, 
                               scalar_basis: torch.Tensor,
                               src: torch.Tensor, 
                               dst: torch.Tensor) -> torch.Tensor:
    r_hat_uns = r_hat.unsqueeze(-1)
    # Project: (E, 3, F) * (E, 3, 1) -> sum -> (E, F)
    p_src = torch.sum(h_node1[src] * r_hat_uns, dim=1)
    p_dst = torch.sum(h_node1[dst] * r_hat_uns, dim=1)
    # Concat
    return torch.cat([scalar_basis, p_src, p_dst], dim=-1)

class PhysicsGating(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        # 1. Chemical Matching (ä¿æŒä¸å˜)
        self.W_query = nn.Linear(self.F, self.F, bias=False)
        self.W_key = nn.Linear(self.F, self.F, bias=False)
        
        # 2. Physical Bias Encoder [å‡çº§]
        # è¾“å…¥ä¸å†æ˜¯ 2F+1ï¼Œè€Œæ˜¯ 3F
        # Scalar_Basis(F) + Proj_Src(F) + Proj_Dst(F) = 3*F
        self.phys_bias_mlp = nn.Sequential(
            nn.Linear(3 * self.F, self.F), # å……åˆ†èåˆ è·ç¦» å’Œ æ–¹å‘
            nn.SiLU(),            
            nn.Linear(self.F, 3 * self.F) 
        )
        
        # 3. Channel Mixer (ä¿æŒä¸å˜)
        self.channel_mixer = nn.Linear(self.F, 3 * self.F, bias=False)
        
        # 4. Gating Scale (ä¿æŒä¸å˜)
        self.gate_scale = nn.Parameter(torch.ones(1) * 2.0)

    # [æ³¨æ„] å‚æ•°åˆ—è¡¨å˜äº†ï¼šæŠŠ d_ij æ¢æˆäº† scalar_basis (å³ basis_edges[0])
    def forward(self, msgs, h_node0, scalar_basis, r_hat, h_node1, edge_index, capture_weights=False):
        if not self.cfg.use_gating: return msgs
        
        src, dst = edge_index
        
        # # --- A. Physical Geometry Features ---
        # if h_node1 is not None:
        #     # æŠ•å½±è®¡ç®— (ä¿æŒä¸å˜)
        #     p_src = torch.sum(h_node1[src] * r_hat.unsqueeze(-1), dim=1, keepdim=False)
        #     p_dst = torch.sum(h_node1[dst] * r_hat.unsqueeze(-1), dim=1, keepdim=False)
        #     p_ij = torch.cat([p_src, p_dst], dim=-1) # (E, 2F)
        # else:
        #     p_ij = torch.zeros((scalar_basis.shape[0], 2 * self.F), device=scalar_basis.device)
            
        # # [å…³é”®å‡çº§]
        # # ä½¿ç”¨ RBF ä¸°å¯Œè¿‡çš„ scalar_basis (E, F) ä»£æ›¿ d_ij (E, 1)
        # # ç°åœ¨çš„ phys_input åŒ…å«äº†ä¸°å¯Œçš„è·ç¦»éçº¿æ€§ä¿¡æ¯
        # # Input: (E, F + 2F) = (E, 3F)
        # phys_input = torch.cat([scalar_basis, p_ij], dim=-1)

        # --- A. Physical Geometry Features ---
        if h_node1 is not None:
            # ğŸ”¥ è°ƒç”¨ JIT å‡½æ•°
            phys_input = compute_gating_projections(h_node1, r_hat, scalar_basis, src, dst)
        else:
            p_ij = torch.zeros((scalar_basis.shape[0], 2 * self.F), device=scalar_basis.device)
            phys_input = torch.cat([scalar_basis, p_ij], dim=-1)

        # --- B. Compute Gating Scores ---
        
        # 1. Chemical (ä¿æŒä¸å˜)
        q = self.W_query(h_node0[dst]) 
        k = self.W_key(h_node0[src])   
        chem_score = q * k             
        chem_logits = self.channel_mixer(chem_score)
        
        # 2. Physical (ç°åœ¨æ›´å¼ºäº†)
        phys_logits = self.phys_bias_mlp(phys_input)
        
        # 3. Fuse & Apply (ä¿æŒä¸å˜)
        raw_gates = chem_logits + phys_logits
        gates = torch.sigmoid(raw_gates) * self.gate_scale
        
        if capture_weights: self.stored_attention = gates.detach()

        g_list = torch.split(gates, self.F, dim=-1)
        g0, g1, g2 = [g.contiguous() for g in g_list]
        
        out_msgs: Dict[int, torch.Tensor] = {}
        if msgs[0] is not None: out_msgs[0] = msgs[0] * g0
        if msgs[1] is not None: out_msgs[1] = msgs[1] * g1.unsqueeze(1)
        if msgs[2] is not None: out_msgs[2] = msgs[2] * g2.unsqueeze(1).unsqueeze(1)
            
        return out_msgs



# ==========================================
# 2. äº¤äº’æ¨¡å— (Interaction Block) - éƒ¨åˆ† JIT åŒ–
# ==========================================

# ğŸ”¥ [JIT å‡½æ•°] è®¡ç®—æ—‹è½¬ä¸å˜é‡ (Safe Norm)
# ğŸ”¥ [ä¿®æ­£ç‰ˆ] JIT å‡½æ•°
@torch.jit.script
def compute_invariants(den0: Optional[torch.Tensor], 
                       den1: Optional[torch.Tensor], 
                       den2: Optional[torch.Tensor]) -> torch.Tensor:
    
    # âœ… ä¿®æ­£åçš„å†™æ³• (Python 3 æ ‡å‡†å†™æ³•):
    invariants: List[torch.Tensor] = []
    
    # L=0
    if den0 is not None:
        invariants.append(den0)
        
    # L=1
    if den1 is not None:
        sq_sum = torch.sum(den1.pow(2), dim=1) 
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    # L=2
    if den2 is not None:
        sq_sum = torch.sum(den2.pow(2), dim=(1, 2))
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    # Concat
    if len(invariants) > 0:
        return torch.cat(invariants, dim=-1)
    else:
        return torch.empty(0)

class CartesianDensityBlock(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.F = config.hidden_dim
        self.cfg = config
        
        # ---------------------------------------------------------
        # 1. ç»´åº¦è®¡ç®—
        # ---------------------------------------------------------
        in_dim = 0
        if config.use_L0: in_dim += self.F
        if config.use_L1: in_dim += self.F
        if config.use_L2: in_dim += self.F 
        
        # ---------------------------------------------------------
        # 2. æ ‡é‡æ›´æ–°ç½‘ç»œ (The "Brain") - è´Ÿè´£å¤„ç†åŒ–å­¦æ€§è´¨
        # ---------------------------------------------------------
        self.scalar_update_mlp = nn.Sequential(
            nn.Linear(in_dim, self.F),
            nn.SiLU(),
            nn.Linear(self.F, self.F)
        )

        # ---------------------------------------------------------
        # 3. [æ”¹è¿›] çŸ¢é‡/å¼ é‡é€šé“æ··åˆå±‚ (Channel Mixing)
        # ---------------------------------------------------------
        # æ³¨æ„ï¼šå¿…é¡»è®¾ç½® bias=False ä»¥ä¿è¯æ—‹è½¬ç­‰å˜æ€§ï¼
        # è¿™å…è®¸ç‰¹å¾ i çš„ç”µæ€§è´¨å»ä¿®æ­£ç‰¹å¾ j çš„ç£æ€§è´¨ã€‚
        if config.use_L1:
            self.L1_linear = nn.Linear(self.F, self.F, bias=False)
        
        if config.use_L2:
            self.L2_linear = nn.Linear(self.F, self.F, bias=False)

        # ---------------------------------------------------------
        # 4. çŸ¢é‡/å¼ é‡ç¼©æ”¾ç½‘ç»œ (The "Valve")
        # ---------------------------------------------------------
        scale_out_dim = 0
        if config.use_L1: scale_out_dim += self.F
        if config.use_L2: scale_out_dim += self.F
        
        if scale_out_dim > 0:
            self.scale_mlp = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, scale_out_dim) # è¾“å‡ºç¼©æ”¾ç³»æ•° alpha
            )
        else:
            self.scale_mlp = None

        # 5. æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
        self.inv_sqrt_deg = 1.0 / (50.0 ** 0.5)

    def forward(self, msgs, index, num_nodes):
        # ===========================
        # 1. å¯†åº¦èšåˆ (Aggregation)
        # ===========================
        densities: Dict[int, Optional[torch.Tensor]] = {}
        for l in [0, 1, 2]:
            if msgs[l] is not None:
                agg = scatter_add(msgs[l], index, dim=0, dim_size=num_nodes)
                densities[l] = agg * self.inv_sqrt_deg 
            else:
                densities[l] = None

        # ===========================
        # 2. [æ”¹è¿›] æå–æ—‹è½¬ä¸å˜é‡ (Invariants)
        # ===========================
        # ä½¿ç”¨ Safe Norm (sqrt(sum^2 + eps)) ä»£æ›¿åŸæ¥çš„ pow(2)
        # ä¼˜åŠ¿ï¼šçº¿æ€§æ¢¯åº¦ï¼Œé˜²æ­¢å°ä¿¡å·æ¢¯åº¦æ¶ˆå¤±ï¼Œé˜²æ­¢å¤§ä¿¡å·æ¢¯åº¦çˆ†ç‚¸
        invariants = []
        
        # # --- L=0 Invariant ---
        # if densities[0] is not None:
        #     invariants.append(densities[0]) 
            
        # # --- L=1 Invariant ---
        # if densities[1] is not None:
        #     # densities[1]: (N, 3, F)
        #     sq_sum = torch.sum(densities[1].pow(2), dim=1) # (N, F)
        #     norm = torch.sqrt(sq_sum + 1e-8)               # Safe Sqrt
        #     invariants.append(norm)
            
        # # --- L=2 Invariant ---
        # if densities[2] is not None:
        #     # densities[2]: (N, 3, 3, F)
        #     sq_sum = torch.sum(densities[2].pow(2), dim=(1, 2)) # (N, F)
        #     norm = torch.sqrt(sq_sum + 1e-8)                    # Safe Sqrt
        #     invariants.append(norm)
        concat = compute_invariants(densities[0], densities[1], densities[2])
        # ===========================
        # 3. è®¡ç®—æ ‡é‡æ›´æ–° (Scalar Update)
        # ===========================
        if invariants:
            # concat = torch.cat(invariants, dim=-1)
            delta_h0 = self.scalar_update_mlp(concat) # (N, F)
        else:
            delta_h0 = torch.zeros((num_nodes, self.F), device=index.device)
        # ===========================
        # 4. [æ”¹è¿›] è®¡ç®—çŸ¢é‡/å¼ é‡æ›´æ–° (Gated Vector Update)
        # ===========================
        delta_h1 = None
        delta_h2 = None

        if self.scale_mlp is not None:
            # ç”¨â€œå¤§è„‘â€æ€è€ƒå‡ºæ¥çš„ delta_h0 æ¥å†³å®šâ€œè‚¢ä½“â€åŠ¨ä½œçš„å¹…åº¦
            scales = self.scale_mlp(delta_h0) # (N, F_L1 + F_L2)
            
            curr_dim = 0
            
            # --- L=1 Update ---
            if self.cfg.use_L1 and densities[1] is not None:
                # 1. è·å–é—¨æ§ç³»æ•°
                alpha1 = scales[:, curr_dim : curr_dim + self.F] 
                
                # 2. [å…³é”®æ”¹è¿›] çº¿æ€§ç‰¹å¾æ··åˆ
                # densities[1]: (N, 3, F) -> Linear -> (N, 3, F)
                # Linear åªä½œç”¨äº F ç»´åº¦ï¼Œä¸ç ´åç©ºé—´ç»“æ„
                h1_mixed = self.L1_linear(densities[1])
                
                # 3. åº”ç”¨é—¨æ§ (Gating)
                # (N, 3, F) * (N, 1, F)
                delta_h1 = h1_mixed * alpha1.unsqueeze(1)
                
                curr_dim += self.F
                
            # --- L=2 Update ---
            if self.cfg.use_L2 and densities[2] is not None:
                # 1. è·å–é—¨æ§ç³»æ•°
                alpha2 = scales[:, curr_dim : curr_dim + self.F]
                
                # 2. [å…³é”®æ”¹è¿›] çº¿æ€§ç‰¹å¾æ··åˆ
                h2_mixed = self.L2_linear(densities[2])
                
                # 3. åº”ç”¨é—¨æ§
                # (N, 3, 3, F) * (N, 1, 1, F)
                delta_h2 = h2_mixed * alpha2.unsqueeze(1).unsqueeze(1)

        return delta_h0, delta_h1, delta_h2

# ==========================================
# 6. é•¿ç¨‹åœº (Latent Long Range) - Ablation Ready
# ==========================================
class LatentLongRange(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        # --- æ¨¡å— 1: éšå¼ç”µè· (Charge) ---
        # ä»æ ‡é‡ç‰¹å¾ h0 é¢„æµ‹ç”µè· q (N, 1)
        if config.use_charge:
            self.q_proj = nn.Sequential(
                nn.Linear(self.F, self.F // 2),
                nn.SiLU(),
                nn.Linear(self.F // 2, 1)
            )
            
        # --- æ¨¡å— 2: éšå¼èŒƒå¾·å (Van der Waals) ---
        # ä»æ ‡é‡ç‰¹å¾ h0 é¢„æµ‹è‰²æ•£ç³»æ•° C6 (N, 1)
        if config.use_vdw:
            self.c6_proj = nn.Sequential(
                nn.Linear(self.F, self.F // 2),
                nn.SiLU(),
                nn.Linear(self.F // 2, 1)
            )
            
        # --- æ¨¡å— 3: éšå¼å¶æ (Dipole) ---
        # ä»çŸ¢é‡ç‰¹å¾ h1 é¢„æµ‹å¶æçŸ© mu (N, 3)
        if config.use_dipole:
            # è¾“å…¥æ˜¯ (N, 3, F)ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ F ç»´åº¦åšçº¿æ€§å˜æ¢
            self.mu_proj = nn.Linear(self.F, 1, bias=False)

    def forward(self, h0, h1, pos, batch):
        """
        h0: (N, F) æ ‡é‡ç‰¹å¾
        h1: (N, 3, F) çŸ¢é‡ç‰¹å¾
        pos: (N, 3) åæ ‡
        batch: (N,) æ‰¹æ¬¡ç´¢å¼•
        """
        energy_total = 0.0
        
        # 1. æ„å»ºå…¨è¿æ¥è·ç¦»çŸ©é˜µ (Full Pairwise Distance)
        # å¯¹äº < 3000 åŸå­çš„ä½“ç³»ï¼Œè¿™æ¯”æ„å»ºé‚»å±…è¡¨æ›´å¿«ä¸”æ›´å‡†
        # mask å¤„ç†ä¸åŒ batch ä¹‹é—´çš„æ— æ•ˆè¿æ¥
        batch_mask = (batch.unsqueeze(1) == batch.unsqueeze(0)) # (N, N)
        
        diff = pos.unsqueeze(1) - pos.unsqueeze(0) # (N, N, 3)
        # åŠ ä¸€ä¸ª epsilon é˜²æ­¢é™¤é›¶
        dist_sq = torch.sum(diff**2, dim=-1) # (N, N)
        dist = torch.sqrt(dist_sq + 1e-8)
        
        # æ’é™¤è‡ªç›¸äº’ä½œç”¨ (å¯¹è§’çº¿)
        diag_mask = torch.eye(dist.size(0), device=dist.device, dtype=torch.bool)
        valid_mask = batch_mask & (~diag_mask)
        
        # ==========================================
        # Ablation 1: ç”µè·å®ˆæ’åº“ä»‘ä½œç”¨ (Coulomb)
        # ==========================================
        if self.cfg.use_charge:
            q = self.q_proj(h0) # (N, 1)
            
            # [å…³é”®] å¼ºåˆ¶ç”µè·ä¸­æ€§ (Charge Neutrality)
            # æ¯ä¸ª batch å†…çš„ç”µè·å’Œå¿…é¡»ä¸º 0
            # ä½¿ç”¨ scatter_add è®¡ç®—æ¯ä¸ª batch çš„æ€»ç”µè·
            from torch_scatter import scatter_add, scatter_mean
            batch_q_mean = scatter_mean(q, batch, dim=0) # (B, 1)
            q = q - batch_q_mean[batch] # ä¸­å¿ƒåŒ–
            
            # è®¡ç®—èƒ½é‡: E = q_i * q_j / r
            # switch: ä½¿ç”¨ damping function é¿å…çŸ­ç¨‹å¥‡å¼‚æ€§ï¼Œå¹¶å¹³æ»‘è¿‡æ¸¡ GNN
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„ soft-core: 1 / sqrt(r^2 + 1) æˆ–è€… damping
            # ç®€å•èµ·è§ï¼Œå‡è®¾ GNN å¤„ç†äº†çŸ­ç¨‹ï¼Œæˆ‘ä»¬åªåŠ é•¿ç¨‹
            
            qq = q @ q.t() # (N, N)
            # Taper function: è®©é•¿ç¨‹åŠ›åœ¨çŸ­ç¨‹ (æ¯”å¦‚ < 4A) æ…¢æ…¢æ¶ˆå¤±
            # f_taper = 1 - exp(-a * r)
            f_taper = 1.0 - torch.exp(-0.5 * dist) 
            
            E_coul = torch.sum(qq / dist * f_taper * valid_mask)
            energy_total += 0.5 * E_coul * 14.399 # 14.399 æ˜¯ eV*A çš„è½¬æ¢ç³»æ•°
            
        # ==========================================
        # Ablation 2: èŒƒå¾·åè‰²æ•£ (Dispersion)
        # ==========================================
        if self.cfg.use_vdw:
            # C6 å¿…é¡»ä¸ºæ­£æ•°ï¼Œä½¿ç”¨ Softplus
            c6 = F.softplus(self.c6_proj(h0)) # (N, 1) softplus ä¸º ln(1 + exp(x)) 
            
            # ç»„åˆè§„åˆ™: C6_ij = sqrt(C6_i * C6_j)
            c6_ij = torch.sqrt(c6 @ c6.t())
            
            # E_vdw = - C6_ij / (r^6 + r_vdw^6)
            # é˜²æ­¢ r->0 æ—¶çˆ†ç‚¸
            r6 = dist_sq ** 3
            damp_r6 = 20.0 # ç»éªŒå€¼ï¼Œæˆ–è€…è®¾ä¸ºå¯å­¦ä¹ å‚æ•°
            
            E_vdw = -torch.sum(c6_ij / (r6 + damp_r6) * valid_mask)
            energy_total += 0.5 * E_vdw

        # ==========================================
        # Ablation 3: å¶æ-å¶æ (Dipole-Dipole)
        # ==========================================
        if self.cfg.use_dipole and h1 is not None:
            # h1: (N, 3, F) -> projection -> (N, 3, 1) -> (N, 3)
            mu = self.mu_proj(h1).squeeze(-1)
            
            # E_dip = (mu_i . mu_j) / r^3 - 3 (mu_i . r)(mu_j . r) / r^5
            
            # 1. mu_i . mu_j
            mu_dot_mu = mu @ mu.t() # (N, N)
            
            # 2. æ–¹å‘å‘é‡ n_ij = r_ij / r
            # è¿™éƒ¨åˆ†è®¡ç®—æ¯”è¾ƒè´¹æ˜¾å­˜ï¼Œå¦‚æœä½ æ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥å…ˆåªè·‘ Charge å’Œ VdW
            n_ij = diff / (dist.unsqueeze(-1) + 1e-8) # (N, N, 3)
            
            # mu . n
            # (N, 1, 3) * (N, N, 3) -> (N, N)
            mu_dot_n_i = torch.sum(mu.unsqueeze(1) * n_ij, dim=-1)
            mu_dot_n_j = torch.sum(mu.unsqueeze(0) * n_ij, dim=-1) # æ³¨æ„ n_ji = -n_ij
            
            term1 = mu_dot_mu # (N, N)
            # æ³¨æ„ç¬¦å·ï¼šn_ji = -n_ijï¼Œæ‰€ä»¥ç¬¬äºŒé¡¹å®é™…ä¸Šæ˜¯ +3? 
            # æ ‡å‡†å…¬å¼: (mu1.mu2)/r3 - 3(mu1.n)(mu2.n)/r3
            term2 = -3 * mu_dot_n_i * mu_dot_n_j # è¿™é‡Œ n_ij æ˜¯åå¯¹ç§°çš„ï¼Œè¦æ³¨æ„
            # è¿™é‡Œç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç”¨ç»å¯¹çš„è·ç¦»å‘é‡è®¡ç®—ï¼Œç‰©ç†ä¸Šä¼šæ›´ä¸¥è°¨
            
            # ç®€åŒ–çš„ damping 1/r^3
            inv_r3 = 1.0 / (dist_sq * dist + 10.0) # damping
            
            E_dip = torch.sum((term1 + term2) * inv_r3 * valid_mask)
            energy_total += 0.5 * E_dip

        return energy_total * self.cfg.long_range_scale