import torch
import torch.nn as nn
import torch.optim as optim
import os
from tqdm.auto import tqdm # âœ… [æ–°å¢] è¿›åº¦æ¡åº“
from Utils import scatter_add # âœ… [æ–°å¢] å¿…é¡»å¯¼å…¥ï¼Œå¦åˆ™ step å‡½æ•°ä¼šæŠ¥é”™

class PotentialTrainer:
    def __init__(self, model, lr=1e-3, device='cuda', checkpoint_dir='checkpoints'):
        self.device = device
        self.model = model.to(self.device)
        
        self.optimizer = optim.AdamW(model.parameters(), lr=lr)
        # è°ƒæ•´äº† patienceï¼Œè®©å®ƒå¯¹ Loss å˜åŒ–æ›´æ•æ„Ÿä¸€ç‚¹
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.75, min_lr=1e-6, patience=10
        )
        
        self.criterion_mse = nn.MSELoss()
        self.criterion_mae = nn.L1Loss()
        
        # æƒé‡
        self.w_e = 1.0
        self.w_f = 100.0
        self.w_s = 0.1 
        
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.EV_A3_TO_GPA = 160.21766 

    def step(self, batch, train=True):
        batch = batch.to(self.device)
        
        # 1. å¼€å¯æ¢¯åº¦ (å¯¹åŸå§‹åæ ‡)
        batch.pos.requires_grad_(True)
        # å¿…é¡»ä¹Ÿå¯¹ Cell å¼€å¯æ¢¯åº¦
        if hasattr(batch, 'cell') and batch.cell is not None:
            batch.cell.requires_grad_(True) 
        
        # ==================================================================
        # A. æ„é€ è™šæ‹Ÿåº”å˜ (Virtual Strain) ç”¨äºè®¡ç®—åº”åŠ›
        # ==================================================================
        num_graphs = batch.batch.max().item() + 1
        
        # åˆ›å»ºä½ç§»æ¢¯åº¦
        displacement = torch.zeros(
            (num_graphs, 3, 3), 
            dtype=batch.pos.dtype, 
            device=self.device
        )
        displacement.requires_grad_(True)
        
        # å¯¹ç§°åŒ–åº”å˜
        symmetric_strain = 0.5 * (displacement + displacement.transpose(-1, -2))
        
        # ==================================================================
        # B. åº”ç”¨å˜å½¢ (Deformation)
        # ==================================================================
        # 1. å˜å½¢åŸå­åæ ‡
        strain_per_atom = symmetric_strain[batch.batch]
        pos_update = torch.einsum('ni,nij->nj', batch.pos, strain_per_atom)
        pos_deformed = batch.pos + pos_update
        
        # 2. å˜å½¢æ™¶èƒ
        if hasattr(batch, 'cell') and batch.cell is not None:
            if batch.cell.dim() == 3: # [Batch, 3, 3]
                 strain_per_cell = symmetric_strain 
                 cell_update = torch.bmm(batch.cell, strain_per_cell) 
                 cell_deformed = batch.cell + cell_update
            else:
                 cell_deformed = batch.cell
        else:
            cell_deformed = None

        # ==================================================================
        # C. æ¨¡å‹å‰å‘
        # ==================================================================
        original_pos = batch.pos
        original_cell = getattr(batch, 'cell', None)
        
        # ä¸´æ—¶æ›¿æ¢ä¸ºå˜å½¢åçš„åæ ‡
        batch.pos = pos_deformed
        if cell_deformed is not None:
            batch.cell = cell_deformed
        
        # Forward
        pred_e = self.model(batch).view(-1)
        
        # æ¢å¤ç°åœº
        batch.pos = original_pos
        if original_cell is not None:
            batch.cell = original_cell
        
        # ==================================================================
        # D. è‡ªåŠ¨æ±‚å¯¼è®¡ç®—åŠ›ä¸åº”åŠ›
        # ==================================================================
        grad_out = torch.ones_like(pred_e)
        
        # æ±‚å¯¼å¯¹è±¡: [åŸå§‹åæ ‡, è™šæ‹Ÿä½ç§»]
        grads = torch.autograd.grad(
            outputs=pred_e, 
            inputs=[original_pos, displacement], 
            grad_outputs=grad_out,
            create_graph=train, 
            retain_graph=train,
            allow_unused=True
        )
        
        pred_f = -grads[0] if grads[0] is not None else torch.zeros_like(batch.pos)
        dE_dStrain = grads[1] # Virial
        
        # è®¡ç®— Stress (Pressureå•ä½): Sigma = Virial / Volume
        if hasattr(batch, 'volume'):
            vol = batch.volume.view(-1, 1, 1)
        else:
            vol = torch.ones_like(dE_dStrain)
            
        pred_stress = dE_dStrain / vol
        
        # ==================================================================
        # E. Loss è®¡ç®—
        # ==================================================================
        target_e = batch.y.view(-1)
        # ä½¿ç”¨ scatter_add è®¡ç®—æ¯ä¸ªå›¾çš„åŸå­æ•°
        num_atoms = scatter_add(torch.ones_like(batch.batch, dtype=torch.float), batch.batch, dim=0).view(-1).clamp(min=1)
        
        # Energy Loss (Per Atom)
        loss_e = self.criterion_mse(pred_e / num_atoms, target_e / num_atoms)
        
        # Force Loss
        loss_f = self.criterion_mse(pred_f, batch.force)
        
        # Stress Loss (å¸¦ Mask)
        if hasattr(batch, 'stress') and batch.stress is not None:
            stress_norm = torch.norm(batch.stress.view(num_graphs, -1), dim=1)
            stress_mask = (stress_norm > 1e-6).float() 
            
            if stress_mask.sum() > 0:
                stress_sq_diff = (pred_stress - batch.stress)**2
                loss_s = (stress_sq_diff.mean(dim=(1, 2)) * stress_mask).sum() / (stress_mask.sum() + 1e-6)
            else:
                loss_s = torch.tensor(0.0, device=self.device, requires_grad=train)
        else:
            loss_s = torch.tensor(0.0, device=self.device, requires_grad=train)

        total_loss = self.w_e * loss_e + self.w_f * loss_f + self.w_s * loss_s
        
        # ==================================================================
        # Metrics
        # ==================================================================
        with torch.no_grad():
            mae_e = self.criterion_mae(pred_e / num_atoms, target_e / num_atoms).item()
            mae_f = self.criterion_mae(pred_f, batch.force).item()
            
            mae_s_gpa = 0.0
            if hasattr(batch, 'stress') and batch.stress is not None:
                if stress_mask.sum() > 0:
                    mae_s_val = (torch.abs(pred_stress - batch.stress).mean(dim=(1,2)) * stress_mask).sum() / stress_mask.sum()
                    mae_s_gpa = mae_s_val.item() * self.EV_A3_TO_GPA

        # Optimization
        if train:
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
        return total_loss.item(), mae_e, mae_f, mae_s_gpa

    # ==================================================================
    # ğŸ”¥ ä¿®æ”¹åçš„ train_epochï¼šæ”¯æŒ total_steps è¿›åº¦æ¡
    # ==================================================================
    def train_epoch(self, loader, total_steps=None):
        self.model.train()
        
        # ç´¯åŠ å™¨
        metrics = {'total': 0, 'mae_e': 0, 'mae_f': 0, 'mae_s': 0}
        count = 0
        
        # ä½¿ç”¨ tqdm åŒ…è£… loader
        # total=total_steps å‘Šè¯‰è¿›åº¦æ¡ç»ˆç‚¹åœ¨å“ªé‡Œ
        pbar = tqdm(loader, total=total_steps, desc="Training", leave=False)
        
        for batch in pbar:
            l, me, mf, ms = self.step(batch, train=True)
            
            metrics['total'] += l
            metrics['mae_e'] += me
            metrics['mae_f'] += mf
            metrics['mae_s'] += ms
            count += 1
            
            # å®æ—¶æ›´æ–°è¿›åº¦æ¡åç¼€
            pbar.set_postfix({
                'Loss': f"{l:.4f}", 
                'MAE_F': f"{mf*1000:.3f}"
            })
            
        # é˜²æ­¢ count ä¸º 0 (ä¾‹å¦‚æ•°æ®é›†ä¸ºç©º)
        if count == 0: count = 1
            
        return {k: v/count for k, v in metrics.items()}

    # ==================================================================
    # ğŸ”¥ ä¿®æ”¹åçš„ validateï¼šæ”¯æŒ total_steps è¿›åº¦æ¡
    # ==================================================================
    def validate(self, loader, total_steps=None):
        self.model.eval()
        
        metrics = {'total': 0, 'mae_e': 0, 'mae_f': 0, 'mae_s': 0}
        count = 0
        
        pbar = tqdm(loader, total=total_steps, desc="Validating", leave=False)
        
        # éªŒè¯æ—¶å¼€å¯æ¢¯åº¦ç”¨äºè®¡ç®— Forceï¼Œä½†ä¸éœ€è¦åå‘ä¼ æ’­ä¼˜åŒ–
        # ä½¿ç”¨ torch.set_grad_enabled(True) ç¡®ä¿ step å‡½æ•°é‡Œçš„ autograd.grad èƒ½å·¥ä½œ
        with torch.set_grad_enabled(True): 
            for batch in pbar:
                # ä¼ å…¥ train=Falseï¼Œè¿™æ · optimizer ä¸ä¼š step
                l, me, mf, ms = self.step(batch, train=False)
                
                metrics['total'] += l
                metrics['mae_e'] += me
                metrics['mae_f'] += mf
                metrics['mae_s'] += ms
                count += 1
        
        if count == 0: count = 1
        avg_metrics = {k: v/count for k, v in metrics.items()}
        
        # æ ¹æ®éªŒè¯é›†çš„ Force MAE è°ƒæ•´å­¦ä¹ ç‡
        self.scheduler.step(avg_metrics['mae_f'])
        
        return avg_metrics

    def save(self, filename='best_model.pt'):
        path = os.path.join(self.checkpoint_dir, filename)
        torch.save(self.model.state_dict(), path)
