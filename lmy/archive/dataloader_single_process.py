import os
import torch
from compute_average_e0 import compute_average_e0
from extxyz_to_pyg_custom import extxyz_to_pyg_custom
from tqdm.auto import tqdm
from torch.utils.data import random_split
from torch_geometric.loader import DataLoader

# ==========================================
# 0. é¢„è®¾ï¼šè¯·ç¡®ä¿ä½ çš„è¾…åŠ©å‡½æ•°å·²å®šä¹‰
# ==========================================
# å‡è®¾ extxyz_to_pyg_custom(file_path, cutoff, topo_abalation) å·²ç»å®šä¹‰
# å‡è®¾ compute_average_e0(dataset) å·²ç»å®šä¹‰

# --- è¾…åŠ©å‡½æ•°ï¼šæ‰¹é‡è¯»å– ---
def load_dataset_from_files(file_paths, cutoff=6.0):
    dataset = []
    print(f"Loading {len(file_paths)} files...")
    for file_path in tqdm(file_paths):
        # è·³è¿‡ç©ºæ–‡ä»¶
        if os.path.getsize(file_path) == 0:
            print(f"âš ï¸ [è·³è¿‡] ç©ºæ–‡ä»¶: {os.path.basename(file_path)}")
            continue
        
        # è°ƒç”¨ä½ çš„è½¬æ¢å‡½æ•°
        data_list = extxyz_to_pyg_custom(file_path, cutoff=cutoff)
        dataset.extend(data_list)
    return dataset

# ==========================================
# 1. ç”¨æˆ·é…ç½®åŒº
# ==========================================
# for linux
file_dir_1 = r"005" # C:\Users\1\Desktop\traIning set\zip_files\005_part1
file_dir_2 = r"outcar_selected_xyz"
file_dir_3 = r"xyz_grouped"
all_files = [os.path.join(file_dir_1, f) for f in os.listdir(file_dir_1) if f.endswith('.xyz')] + \
            [os.path.join(file_dir_2, f) for f in os.listdir(file_dir_2) if f.endswith('.xyz')] + \
            [os.path.join(file_dir_3, f) for f in os.listdir(file_dir_3) if f.endswith('.xyz')]
# for windows
import random
random.shuffle(all_files)


# file_dir_1 = r"C:\Users\1\Desktop\traIning set\AIMD_selected_xyz\outcar_selected_xyz" # C:\Users\1\Desktop\traIning set\zip_files\005_part1
# file_dir_2 = r"C:\Users\1\Desktop\traIning set\zip_files\005_part1"

# all_files = [os.path.join(file_dir_1, f) for f in os.listdir(file_dir_1) if f.endswith('.xyz')] + \
#             [os.path.join(file_dir_2, f) for f in os.listdir(file_dir_2)[0:8000] if f.endswith('.xyz')]
#æ‰“ä¹±
import random
random.shuffle(all_files)

# æ¨¡å¼é€‰æ‹©
SPLIT_MODE = 'manual' 
# SPLIT_MODE = 'random' 
BATCH_SIZE = 8

# --- ğŸ’¾ ä¿å­˜æ§åˆ¶å¼€å…³ ---
IS_SAVE = True               # è®¾ç½®ä¸º True åˆ™ä¿å­˜ï¼ŒFalse åˆ™ä¸ä¿å­˜
SAVE_PATH = "processed_dataset.pt"  # ä¿å­˜çš„æ–‡ä»¶å

# ==========================================
# 2. æ•°æ®åŠ è½½ä¸åˆ’åˆ†
# ==========================================
if SPLIT_MODE == 'manual':
    print(">>> ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶åˆ’åˆ†æ•°æ®é›†")
    train_files = all_files[:20000]  
    test_files = all_files[20000:21000]
    train_set = load_dataset_from_files(train_files)
    test_set = load_dataset_from_files(test_files)

elif SPLIT_MODE == 'random':
    print(">>> ä½¿ç”¨éšæœºæ¯”ä¾‹åˆ’åˆ†æ•°æ®é›† (Train: 90%, Test: 10%)")
    full_dataset = load_dataset_from_files(all_files)
    total_len = len(full_dataset)
    train_len = int(0.9 * total_len)
    test_len = total_len - train_len
    
    train_set, test_set = random_split(
        full_dataset, 
        [train_len, test_len], 
        generator=torch.Generator().manual_seed(42)
    )

print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: Train={len(train_set)}, Test={len(test_set)}")

# ==========================================
# 3. è®¡ç®—åŸå­å¹³å‡èƒ½é‡ (E0)
# ==========================================
print("è®¡ç®—åŸå­å¹³å‡èƒ½é‡ (E0)...")
e0_dict = compute_average_e0(train_set)
print(f"E0 è®¡ç®—å®Œæˆ: {e0_dict}")

# ==========================================
# 4. ä¿å­˜å¤„ç†åçš„æ•°æ® (æ ¹æ® IS_SAVE åˆ¤æ–­)
# ==========================================
if IS_SAVE:
    print(f"ğŸ’¾ å¼€å…³å·²æ‰“å¼€ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®åˆ° {SAVE_PATH} ...")
    
    data_to_save = {
        'train_set': train_set,
        'test_set': test_set,
        'e0_dict': e0_dict,
        'config': {'split_mode': SPLIT_MODE, 'train_source': train_files, 'test_source': test_files}
    }
    
    try:
        torch.save(data_to_save, SAVE_PATH)
        print("ğŸ‰ ä¿å­˜æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
else:
    print("â© IS_SAVE ä¸º Falseï¼Œè·³è¿‡ä¿å­˜æ­¥éª¤ã€‚")

# ==========================================
# 5. æ„å»º DataLoader
# ==========================================
train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)

print("ğŸš€ Loader æ„å»ºå®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒï¼")
