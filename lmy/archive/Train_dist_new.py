import torch
import numpy as np
import json
import os
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch_geometric.loader import DataLoader

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from src.data import ChunkedSmartDataset   # ğŸ‘ˆ æ–°çš„æ•°æ®é›†
from src.data import BinPackingSampler # ğŸ‘ˆ æ–°çš„é‡‡æ ·å™¨
from src.models import HTGPModel
from src.utils import HTGPConfig
from src.engine import PotentialTrainer 

torch.multiprocessing.set_sharing_strategy('file_system')

# âœ… æ”¹ä¸º Float32
torch.set_default_dtype(torch.float32)

# âœ… ğŸš€ å¼€å¯ TF32 (ä½ çš„ H100/A100 æ˜¾å¡ç¥å™¨ï¼Œç²¾åº¦å‡ ä¹ä¸æ‰ï¼Œé€Ÿåº¦å¿«å¾ˆå¤š)
torch.backends.cuda.matmul.allow_tf32 = True 
torch.backends.cudnn.allow_tf32 = True
# ==========================================
# 1. DDP åˆå§‹åŒ–
# ==========================================
def init_distributed_mode():
    if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://", world_size=world_size, rank=rank)
        dist.barrier()
        if rank == 0: print(f"ğŸš€ [Init] DDP Enabled: Rank {rank}/{world_size}")
        return local_rank, rank, world_size
    else:
        print("âš ï¸ Single GPU Mode")
        return 0, 0, 1

LOCAL_RANK, RANK, WORLD_SIZE = init_distributed_mode()
DEVICE = torch.device(f"cuda:{LOCAL_RANK}")
print(f"Using device: {DEVICE}")

# ==========================================
# 2. å‚æ•°é…ç½®
# ==========================================
DATA_DIR = "/var/tmp/lmy_test/"  # ğŸ‘ˆ ç¡®ä¿è¿™é‡Œæœ‰ train_metadata.pt
LOG_DIR = "Checkpoints"
meta_e0_path = "/var/tmp/lmy_test/meta_e0_data.pt"
# ğŸ”¥ è¿™é‡Œçš„ BATCH_SIZE å˜sæˆäº† "æ¯ä¸ª Batch çš„æœ€å¤§åŸå­æ•°"
MAX_COST_PER_BATCH = 8100  # H100
NUM_WORKERS = 6            # æ ¹æ®ä½ çš„ CPU æ ¸æ•°è°ƒæ•´
LR = 1e-3
EPOCHS = 15

if RANK == 0: os.makedirs(LOG_DIR, exist_ok=True)
os.environ["OMP_NUM_THREADS"] = "1"   
# ==========================================
# 3. å‡†å¤‡æ•°æ® (Loader)
# ==========================================
if RANK == 0: print("\n[1/5] Initializing Smart DataLoaders...")

# --- A. è®­ç»ƒé›† (ä½¿ç”¨è£…ç®±é‡‡æ ·) ---
try:
    train_dataset = ChunkedSmartDataset(
        DATA_DIR, 
        metadata_file="train_metadata.pt", 
        cache_size=2, # ç¼“å­˜ 16 ä¸ªæ–‡ä»¶å—
        rank=RANK,
        world_size=WORLD_SIZE
    )
except FileNotFoundError:
    raise FileNotFoundError(f"âŒ æ²¡æ‰¾åˆ° train_metadata.ptï¼Œè¯·å…ˆè¿è¡Œ preprocess.pyï¼")

train_sampler = BinPackingSampler(
    train_dataset.metadata,
    max_cost=MAX_COST_PER_BATCH,
    edge_weight="auto", # è¾¹æƒé‡
    shuffle=True,
    world_size=WORLD_SIZE,
    rank=RANK
)

# print("test æ‰“å°å‰10ä¸ªæ ·æœ¬çš„ç´¢å¼•å’Œæˆæœ¬", train_sampler.indices_with_cost[:10])  # æ‰“å°å‰10ä¸ªæ ·æœ¬çš„ç´¢å¼•å’Œæˆæœ¬ï¼Œæ£€æŸ¥é‡‡æ ·å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œ

train_loader = DataLoader(
    train_dataset,
    batch_sampler=train_sampler, # ğŸ‘ˆ å…³é”®ï¼šä½¿ç”¨ batch_sampler
    num_workers=NUM_WORKERS,
    pin_memory=True,
    prefetch_factor=4,
)

# --- B. æµ‹è¯•é›† (ç®€å•åŠ è½½å³å¯) ---
# æµ‹è¯•é›†ä¹Ÿå¯ä»¥ç”¨è£…ç®±æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†ä¸ç”¨ shuffle
try:
    test_dataset = ChunkedSmartDataset(
        DATA_DIR, 
        metadata_file="test_metadata.pt",
        cache_size=2,
        rank=RANK,
        world_size=WORLD_SIZE
    )
    test_sampler = BinPackingSampler(
        test_dataset.metadata,
        max_cost=MAX_COST_PER_BATCH, # æ¨ç†æ—¶ä¸å­˜æ¢¯åº¦ï¼Œè¿™ä¸ªå€¼å¯ä»¥è®¾å¾—æ¯”è®­ç»ƒå¤§ä¸€å€
        edge_weight="auto",
        shuffle=False,
        world_size=WORLD_SIZE,
        rank=RANK
    )
    test_loader = DataLoader(
        test_dataset,
        batch_sampler=test_sampler,
        num_workers=4,
        prefetch_factor=4,
        pin_memory=True
    )
except FileNotFoundError:
    if RANK == 0: print("âš ï¸ test_metadata.pt not found, skipping validation setup.")
    test_loader = None

# ==========================================
# 4. æ¨¡å‹æ„å»ºä¸ç¼–è¯‘
# ==========================================
if RANK == 0: print("\n[2/5] Building & Compiling Model...")

config = HTGPConfig(
    num_atom_types=55, 
    hidden_dim=64, 
    num_layers=3, 
    cutoff=6.0, 
    num_rbf=10,
    use_L0=True, 
    use_L1=True,
    use_L2=True, 
    use_gating=True, 
    use_long_range=False
)

model = HTGPModel(config).to(DEVICE)
print(f"ğŸ§  Model Parameters: {sum(p.numel() for p in model.parameters())}")

if RANK == 0: print(f"ğŸ§  Model Parameters: {sum(p.numel() for p in model.parameters())}")

# --- æ³¨å…¥ E0 (ä» metadata.pt åŠ è½½) ---
if RANK == 0: print("\n[3/5] Loading Atomic References (E0)...")
if os.path.exists(meta_e0_path):
    meta_data = torch.load(meta_e0_path, map_location='cpu', weights_only=False)
    e0_dict = meta_data.get('e0_dict', None)
    model.load_external_e0(e0_dict)
    count = len(e0_dict) if e0_dict else 0
    model.atomic_ref.weight.requires_grad = False
    if RANK == 0:
        print(f"ğŸ”’ Injected E0 for {count} elements (Float32).")
else:
    model.atomic_ref.weight = model.atomic_ref.weight.float()
    if RANK == 0: print("âš ï¸ meta_e0_data.pt not found, skipping E0 injection.")
    
# DDP Wrap
if dist.is_initialized():
    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, find_unused_parameters=True)

# # ==========================================
# # 5. Trainer åˆå§‹åŒ–
# # ==========================================
# # ç”±äº Sampler æ˜¯åŠ¨æ€çš„ï¼Œæˆ‘ä»¬éœ€è¦ä¼°ç®— Steps
# # å¯ä»¥å…ˆè·‘ä¸€é len(train_sampler) æˆ–è€…ç”¨ä¼°ç®—å€¼
exact_total_steps = train_sampler.precompute_total_steps(EPOCHS)

if RANK == 0: print(f"ğŸ“Š Estimated total steps: {exact_total_steps}")

trainer = PotentialTrainer(
    model, 
    total_steps=exact_total_steps,
    # epochs=EPOCHS,
    max_lr=LR, 
    device=DEVICE, 
    checkpoint_dir=LOG_DIR
)

# ==========================================
# 6. è®­ç»ƒå¾ªç¯
# ==========================================
if RANK == 0: 
    print("\n[4/5] Starting Loop...")
    print("="*80)

for epoch in range(1, EPOCHS + 1):
    # Map-style Dataset é…åˆ Sampler ä¸éœ€è¦æ‰‹åŠ¨ set_epoch
    train_sampler.set_epoch(epoch)
    
    # è®­ç»ƒ
    train_metrics = trainer.train_epoch(train_loader, epoch_idx=epoch)
    
    # éªŒè¯
    if test_loader:
        val_metrics = trainer.validate(test_loader, epoch_idx=epoch)
    else:
        val_metrics = {'total_loss': 0.0, 'mae_e': 0.0, 'mae_f': 0.0, 'mae_s_gpa': 0.0}
    
    if RANK == 0:
        print(f"Ep {epoch} | T_Loss: {train_metrics['total_loss']:.4f} V_Loss: {val_metrics['total_loss']:.4f} | "
              f"MAE_F: {train_metrics['mae_f']*1000:.1f}/{val_metrics['mae_f']*1000:.1f} meV/A")
        
        # ä¿å­˜é€»è¾‘
        trainer.save(f'model_epoch_{epoch}.pt')

    if dist.is_initialized(): # æ‰€æœ‰rankåœ¨è¿™é‡Œç­‰ Rank 0 å†™å®Œæ–‡ä»¶
        dist.barrier()

if dist.is_initialized(): dist.destroy_process_group()