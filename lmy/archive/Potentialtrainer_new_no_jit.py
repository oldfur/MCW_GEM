import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os
import csv
from tqdm.auto import tqdm
from Utils import scatter_add
import torch.distributed as dist
from torch_ema import ExponentialMovingAverage

def conditional_huber_loss(pred, target, base_delta=0.01):
    """
    è‡ªé€‚åº” Huber Loss (Adaptive Huber Loss)ã€‚
    é’ˆå¯¹åŠ›(Force)æ•°æ®è·¨åº¦å¤§çš„ç‰©ç†ç‰¹æ€§è®¾è®¡ã€‚
    
    æœºåˆ¶:
    æ ¹æ®çœŸå®åŠ›(Target Force)çš„æ¨¡é•¿åŠ¨æ€è°ƒæ•´ Huber Loss çš„é˜ˆå€¼(delta)ã€‚
    - å¹³è¡¡æ€(åŠ›å°): ä½¿ç”¨ base_delta, ä¿æŒ MSE çš„é«˜ç²¾åº¦ç‰¹æ€§ã€‚
    - å‰§çƒˆåŠ¨æ€(åŠ›å¤§): å‡å° delta, ä½¿ Loss æ›´æ—©è¿›å…¥ L1 çº¿æ€§åŒº, é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
    
    Args:
        pred: é¢„æµ‹å€¼ (N_atoms, 3)
        target: çœŸå®å€¼ (N_atoms, 3)
        base_delta: åŸºç¡€é˜ˆå€¼, é»˜è®¤ 0.01
    """
    # è®¡ç®—æ¯ä¸ªåŸå­çš„å—åŠ›æ¨¡é•¿ (N, 1)
    force_norm = torch.norm(target, dim=1, keepdim=True)
    
    # åˆå§‹åŒ–ç¼©æ”¾å› å­
    delta_scale = torch.ones_like(force_norm)
    
    # é˜¶æ¢¯å¼é™çº§ç­–ç•¥
    # Force < 100: scale = 1.0
    # 100 <= Force < 200: scale = 0.7
    mask_100_200 = (force_norm >= 100) & (force_norm < 200)
    delta_scale[mask_100_200] = 0.7
    
    # 200 <= Force < 300: scale = 0.4
    mask_200_300 = (force_norm >= 200) & (force_norm < 300)
    delta_scale[mask_200_300] = 0.4
    
    # Force >= 300: scale = 0.1 (æç«¯å€¼ä½¿ç”¨å¼ºé²æ£’æ€§ L1)
    mask_300 = (force_norm >= 300)
    delta_scale[mask_300] = 0.1
    
    # è®¡ç®—æœ€ç»ˆçš„ delta (N, 1) -> å¹¿æ’­åˆ° (N, 3)
    adaptive_delta = base_delta * delta_scale
    
    # æ‰‹åŠ¨å®ç° Huber è®¡ç®—é€»è¾‘
    error = pred - target
    abs_error = torch.abs(error)
    
    # åˆ¤å®š MSE åŒºåŸŸ
    is_mse = abs_error < adaptive_delta
    
    loss_mse = 0.5 * error ** 2
    loss_l1 = adaptive_delta * (abs_error - 0.5 * adaptive_delta)
    
    # ç»„åˆå¹¶å–å¹³å‡
    loss = torch.where(is_mse, loss_mse, loss_l1)
    return loss.mean()

class PotentialTrainer:
    def __init__(self, model, steps_per_epoch, epochs, lr=1e-3, device='cuda', checkpoint_dir='checkpoints'):
        """
        Args:
            steps_per_epoch: æ¯ä¸ª Epoch çš„æ­¥æ•° (ç”¨äº Scheduler è§„åˆ’æ›²çº¿)
            epochs: æ€»è®­ç»ƒè½®æ¬¡
        """
        self.device = device
        self.model = model.to(self.device)
        
        # ------------------------------------------------------------------
        # 1. ä¼˜åŒ–å™¨é…ç½®
        # ä½¿ç”¨ AdamW å¹¶å¼€å¯ AMSGrad ä»¥æå‡æ”¶æ•›ç¨³å®šæ€§
        # Weight Decay è®¾ä¸º 1e-4ï¼Œæä¾›è½»å¾®æ­£åˆ™åŒ–
        # ------------------------------------------------------------------
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=lr, 
            weight_decay=1e-4, 
            amsgrad=True
        )

        # ------------------------------------------------------------------
        # 2. EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡)
        # ç»´æŠ¤ä¸€ä»½å¹³æ»‘çš„æƒé‡å‰¯æœ¬ï¼Œç”¨äºéªŒè¯å’Œæ¨ç†ï¼Œæå¤§æå‡æ³›åŒ–èƒ½åŠ›
        # ------------------------------------------------------------------
        self.ema = ExponentialMovingAverage(self.model.parameters(), decay=0.999)

        # ------------------------------------------------------------------
        # 3. å­¦ä¹ ç‡è°ƒåº¦å™¨ (OneCycleLR)
        # é’ˆå¯¹çŸ­å‘¨æœŸ(Few Epochs)å¤§æ¨¡å‹è®­ç»ƒçš„ç­–ç•¥
        # ------------------------------------------------------------------
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=lr,
            epochs=epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.05,           # 5% æ­¥æ•°ç”¨äºé¢„çƒ­(Warmup)ï¼Œé˜²æ­¢åˆæœŸåŠ›æ¢¯åº¦è¿‡å¤§
            div_factor=100.0,        # åˆå§‹å­¦ä¹ ç‡ä¸º max_lr / 100ï¼Œèµ·æ­¥æ›´ç¨³
            final_div_factor=1000.0, # æœ€ç»ˆè¡°å‡åˆ°æå°å€¼
            anneal_strategy='cos'
        )
        
        # Loss é…ç½®
        self.huber_delta = 0.01  # åŸºç¡€ Delta
        self.w_e = 1.0           # èƒ½é‡æƒé‡
        self.w_f = 10.0          # åŠ›æƒé‡ (é…åˆ Huber Loss ä½¿ç”¨ 10, è‹¥ç”¨ MSE éœ€æ›´å¤§)
        self.w_s = 10.0          # åº”åŠ›æƒé‡
        
        # è·å– rank
        self.rank = dist.get_rank() if dist.is_initialized() else 0
        self.checkpoint_dir = checkpoint_dir
        self.train_log_path = os.path.join(self.checkpoint_dir, 'train_log.csv')
        self.val_log_path = os.path.join(self.checkpoint_dir, 'val_log.csv')
        self.EV_A3_TO_GPA = 160.21766 
        
        # æ—¥å¿—åˆå§‹åŒ–
        if self.rank == 0:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            self._init_loggers()

    def _init_loggers(self):
        headers = ['epoch', 'step', 'lr', 'total_loss', 'loss_e', 'loss_f', 'loss_s', 'mae_e', 'mae_f', 'mae_s_gpa']
        for path in [self.train_log_path, self.val_log_path]:
            # è¦†ç›–æ¨¡å¼åˆå§‹åŒ– (å¦‚éœ€ç»­è®­è¯·æ”¹ä¸º 'a' å¹¶å¢åŠ åˆ¤æ–­)
            with open(path, 'w', newline='') as f:
                csv.writer(f).writerow(headers)

    def log_to_csv(self, mode, data):
        # åªæœ‰ rank 0 å†™å…¥
        if self.rank != 0:
            return
        path = self.train_log_path if mode == 'train' else self.val_log_path
        with open(path, 'a', newline='') as f:
            csv.writer(f).writerow([
                data['epoch'], data['step'], f"{data['lr']:.2e}",
                f"{data['total_loss']:.6f}", f"{data['loss_e']:.6f}",
                f"{data['loss_f']:.6f}", f"{data['loss_s']:.6f}",
                f"{data['mae_e']*1000:.6f} meV/atom", f"{data['mae_f']*1000:.6f} meV", f"{data['mae_s_gpa']:.6f} GPa"
            ])

    def step(self, batch, train=True):
        batch = batch.to(self.device)
        
        # --- 1. å¼€å¯æ¢¯åº¦ (Force & Stress è®¡ç®—æ‰€éœ€) ---
        batch.pos.requires_grad_(True)
        if hasattr(batch, 'cell') and batch.cell is not None:
            batch.cell.requires_grad_(True) 
        
        # --- 2. æ„é€ è™šæ‹Ÿåº”å˜ (Virtual Strain) ---
        num_graphs = batch.batch.max().item() + 1
        displacement = torch.zeros((num_graphs, 3, 3), dtype=batch.pos.dtype, device=self.device)
        displacement.requires_grad_(True)
        symmetric_strain = 0.5 * (displacement + displacement.transpose(-1, -2))
        
        # --- 3. åº”ç”¨å˜å½¢ ---
        strain_per_atom = symmetric_strain[batch.batch]
        pos_deformed = batch.pos + torch.einsum('ni,nij->nj', batch.pos, strain_per_atom)
        
        original_pos = batch.pos
        original_cell = getattr(batch, 'cell', None)
        
        batch.pos = pos_deformed
        
        if original_cell is not None and original_cell.dim() == 3:
            # batch.cell é€šå¸¸æ˜¯ (Batch, 3, 3)
            # è¿™é‡Œçš„ä¹˜æ³•é€»è¾‘å–å†³äºä½ çš„ cell å®šä¹‰æ˜¯è¡Œå‘é‡è¿˜æ˜¯åˆ—å‘é‡ï¼Œé€šå¸¸ ASE/PyG æ˜¯è¡Œå‘é‡
            cell_deformed = original_cell + torch.bmm(original_cell, symmetric_strain)
            batch.cell = cell_deformed
        else:
            print("âš ï¸ Warning: batch.cell is None or not 3D, skipping cell deformation.")
            # ç¨‹åºåœæ­¢åœ¨è¿™é‡Œä»¥é˜²åç»­æŠ¥é”™
            raise ValueError("batch.cell is None or not 3D") 

        # --- 4. å‰å‘ä¼ æ’­ ---
        pred_e = self.model(batch).view(-1)
        
        # æ¢å¤åŸå§‹åæ ‡
        batch.pos = original_pos
        if original_cell is not None: batch.cell = original_cell
        
        # --- 5. è‡ªåŠ¨æ±‚å¯¼è®¡ç®—åŠ›ä¸åº”åŠ› ---
        grad_out = torch.ones_like(pred_e)
        grads = torch.autograd.grad(
            outputs=pred_e, 
            inputs=[original_pos, displacement], 
            grad_outputs=grad_out,
            create_graph=train, 
            retain_graph=train,
            allow_unused=True
        )
        
        pred_f = -grads[0] if grads[0] is not None else torch.zeros_like(batch.pos)
        dE_dStrain = grads[1]

        # --- 6. ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®æ­£ä½“ç§¯è®¡ç®— ğŸ”¥ğŸ”¥ğŸ”¥ ---
        if original_cell is not None:
            # å®æ—¶è®¡ç®—ä½“ç§¯ï¼šVol = det(Cell)
            # torch.linalg.det è®¡ç®—è¡Œåˆ—å¼
            vol = torch.abs(torch.linalg.det(original_cell)) # (Batch,)
            
            # è°ƒæ•´å½¢çŠ¶ä»¥ä¾¿å¹¿æ’­: (Batch,) -> (Batch, 1, 1)
            vol = vol.view(-1, 1, 1)
        else:
            # éå‘¨æœŸæ€§ä½“ç³»ï¼ˆåˆ†å­ï¼‰ï¼Œæ²¡æœ‰ä½“ç§¯å®šä¹‰ï¼Œé€šå¸¸ä¸è®¡ç®— Stress
            vol = torch.ones_like(dE_dStrain)

        pred_stress = dE_dStrain / vol
        
        # ==================================================================
        # 6. Loss è®¡ç®— (ä½¿ç”¨å¢å¼ºç‰ˆ Huber Loss)
        # ==================================================================
        target_e = batch.y.view(-1)
        num_atoms = scatter_add(torch.ones_like(batch.batch, dtype=torch.float64), batch.batch, dim=0).view(-1).clamp(min=1)
        
        # Energy: æ™®é€š Huber
        loss_e = F.huber_loss(pred_e / num_atoms, target_e / num_atoms, delta=self.huber_delta)
        
        # Force: è‡ªé€‚åº” Conditional Huber
        loss_f = conditional_huber_loss(pred_f, batch.force, base_delta=self.huber_delta)
        
        # Stress: æ™®é€š Huber (å¸¦ Mask)
        loss_s = torch.tensor(0.0, device=self.device, requires_grad=train)
        stress_mask_sum = 0
        if hasattr(batch, 'stress') and batch.stress is not None:
            stress_norm = torch.norm(batch.stress.view(num_graphs, -1), dim=1)
            stress_mask = (stress_norm > 1e-6)
            stress_mask_sum = stress_mask.sum().item()
            if stress_mask_sum > 0:
                s_pred = pred_stress.view(num_graphs, -1)[stress_mask]
                s_target = batch.stress.view(num_graphs, -1)[stress_mask]
                loss_s = F.huber_loss(s_pred, s_target, delta=self.huber_delta)

        total_loss = self.w_e * loss_e + self.w_f * loss_f + self.w_s * loss_s
        
        # --- 7. Metrics è®¡ç®— (MAE, ç‰©ç†å•ä½) ---
        with torch.no_grad():
            # ä½¿ç”¨ L1 Loss è®¡ç®— MAE
            mae_e = F.l1_loss(pred_e / num_atoms, target_e / num_atoms).item()
            mae_f = F.l1_loss(pred_f, batch.force).item()
            mae_s_gpa = 0.0
            if stress_mask_sum > 0:
                mae_s_val = F.l1_loss(
                    pred_stress.view(num_graphs, -1)[stress_mask], 
                    batch.stress.view(num_graphs, -1)[stress_mask]
                )
                mae_s_gpa = mae_s_val.item() * self.EV_A3_TO_GPA

        # --- 8. åå‘ä¼ æ’­ä¸ä¼˜åŒ– ---
        if train:
            self.optimizer.zero_grad(set_to_none=True)
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)
            
            self.optimizer.step()
            
            # ğŸ”¥ å…³é”®: æ¯æ¬¡å‚æ•°æ›´æ–°åï¼Œç«‹å³æ›´æ–° EMA å½±å­æƒé‡
            self.ema.update()
            
        return {
            'total_loss': total_loss.item(),
            'loss_e': loss_e.item(), 'loss_f': loss_f.item(), 'loss_s': loss_s.item(),
            'mae_e': mae_e, 'mae_f': mae_f, 'mae_s_gpa': mae_s_gpa
        }

    def train_epoch(self, loader, epoch_idx):
        self.model.train()
        pbar = tqdm(loader, desc=f"Train Ep {epoch_idx}", leave=False, disable=(self.rank != 0))
        current_step = 0
        metrics_sum = {'mae_e': 0, 'mae_f': 0, 'mae_s_gpa': 0, 'total_loss': 0}
        count = 0
        
        for batch in pbar:
            # 1. è®­ç»ƒä¸€æ­¥
            metrics = self.step(batch, train=True)
            # æ‰“å°ç¬¬ä¸€ä¸ªbatchçš„å›¾ï¼ŒåŸå­å’Œè¾¹çš„ç´¢å¼•ä¿¡æ¯éªŒè¯å®ƒä»¬æ˜¯å¦å¯¹åº”çš„æ­£ç¡®æ€§, çœ‹batch.batchæ˜¯å¦å’ŒåŸå­æ•°å¯¹åº”
            if current_step == 0:
                if self.rank == 0:
                    print("First batch graph info:")
                    print("Number of graphs in batch:", batch.num_graphs)
                    print("Nodes (atoms) in batch:", batch.pos.size(0))
                    print("Edge index:", batch.edge_index)
                    print("Batch indices:", batch.batch)
                    # çœ‹stressæ˜¯ä¸æ˜¯ä¸æ˜¯Noneå’Œç©º
                    if hasattr(batch, 'stress') and batch.stress is not None:
                        print("Stress tensor shape:", batch.stress.shape)
                    else:
                        print("No stress tensor in this batch.")
            # 2. è®°å½• CSV
            log_data = metrics.copy()
            log_data.update({'epoch': epoch_idx, 'step': current_step, 'lr': self.optimizer.param_groups[0]['lr']})
            self.log_to_csv('train', log_data)
            
            # 3. ğŸ”¥ Scheduler Step (Batch-level)
            # å¿…é¡»åœ¨æ¯ä¸ª batch åè°ƒç”¨ï¼Œç¡®ä¿ OneCycleLR æ›²çº¿ç”Ÿæ•ˆ
            self.scheduler.step()
            
            # 4. ç»Ÿè®¡
            for k in metrics_sum: metrics_sum[k] += metrics[k]
            count += 1
            current_step += 1
            pbar.set_postfix({'L': f"{metrics['total_loss']:.4f}", 
                              'MAE_e': f"{metrics['mae_e']*1000:.1f}",
                              'MAE_F': f"{metrics['mae_f']*1000:.1f}"})
            
        return {k: v/count for k,v in metrics_sum.items()}

    def validate(self, loader, epoch_idx):
        # éªŒè¯æ—¶ä¸åº”ä½¿ç”¨ train æ¨¡å¼ï¼Œä¹Ÿä¸åº”æ›´æ–°æ¢¯åº¦
        # ä½†å¦‚æœæ˜¯ Graph Norm ç­‰å±‚ï¼Œéœ€æ³¨æ„ eval æ¨¡å¼çš„è¡Œä¸º
        self.model.eval()
        pbar = tqdm(loader, desc=f"Val Ep {epoch_idx}", leave=False, disable=(self.rank != 0))
        metrics_sum = {'mae_e': 0, 'mae_f': 0, 'mae_s_gpa': 0, 'total_loss': 0}
        count = 0
        current_step = 0
        
        # ğŸ”¥ å…³é”®: ä½¿ç”¨ EMA çš„å¹³æ»‘æƒé‡è¿›è¡ŒéªŒè¯ï¼Œé€šå¸¸èƒ½è·å¾—æ›´ä½ä¸”æ›´ç¨³çš„ Error
        with self.ema.average_parameters():
            with torch.set_grad_enabled(True): # å¿…é¡»å¼€å¯ grad æ‰èƒ½è®¡ç®— Force
                for batch in pbar:
                    metrics = self.step(batch, train=False)
                    
                    # è®°å½• CSV
                    log_data = metrics.copy()
                    log_data.update({'epoch': epoch_idx, 'step': current_step, 'lr': self.optimizer.param_groups[0]['lr']})
                    self.log_to_csv('val', log_data)
                    
                    for k in metrics_sum: metrics_sum[k] += metrics[k]
                    count += 1
                    current_step += 1
                    pbar.set_postfix({'L': f"{metrics['total_loss']:.4f}", 
                              'MAE_e': f"{metrics['mae_e']*1000:.1f}",
                              'MAE_F': f"{metrics['mae_f']*1000:.1f}"})
        
        if count == 0: count = 1
        return {k: v/count for k,v in metrics_sum.items()}

    def save(self, filename='best_model.pt'):
        path = os.path.join(self.checkpoint_dir, filename)
        # ä¿å­˜æ—¶ï¼Œæ¨èä¿å­˜ EMA å¤„ç†è¿‡çš„æƒé‡ä½œä¸ºæœ€ä½³æ¨¡å‹
        with self.ema.average_parameters():
            torch.save(self.model.state_dict(), path)
            