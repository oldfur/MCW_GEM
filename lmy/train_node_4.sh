ulimit -n 65535

# 1. æ¡æ‰‹ç½‘å¡ (å¿…é¡»æœ‰)
export NCCL_SOCKET_IFNAME=ens22f0np0

export NCCL_IB_CUDA_SUPPORT=0
# 2. ğŸ”¥ã€è¡¥ä¸Aã€‘é˜²æ­¢èŠ‚ç‚¹å†… PCIe æ­»é” (ä¿ç•™ IBï¼Œä½†ç¦ç”¨ P2P)
export NCCL_P2P_DISABLE=1

# 3. ğŸ”¥ã€è¡¥ä¸Bã€‘å¼ºåˆ¶è¿‡æ»¤ï¼Œåªå…è®¸ IB ç½‘å¡åš RDMA (æ’é™¤ ens å’Œ docker)
export NCCL_IB_HCA=^ens,eth,docker

# 4. ğŸ”¥ã€è¡¥ä¸Cã€‘æ‰“å¼€æ—¥å¿—ï¼Œçœ‹çœ‹åˆ°åº•å¡åœ¨å“ª
export NCCL_DEBUG=INFO

# 5. æ˜¾å­˜ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
unset NCCL_IB_DISABLE

# 6. å¯åŠ¨
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
torchrun \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=0 \
    --rdzv_id=66666 \
    --rdzv_backend=c10d \
    --rdzv_endpoint=192.168.0.5:29500 \
    Train_dist_restart.py
