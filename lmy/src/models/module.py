import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, Optional, Tuple, List
from src.utils import scatter_add, scatter_mean, HTGPConfig

# ==========================================
# ğŸ”¥ æ ¸å¿ƒ JIT æ•°å­¦å¼•æ“ (å®‰å…¨åŠ é€ŸåŒº)
# ==========================================

@torch.jit.script
def compute_bessel_math(d: torch.Tensor, r_max: float, freq: torch.Tensor) -> torch.Tensor:
    d_scaled = d / r_max
    prefactor = (2.0 / r_max) ** 0.5
    return prefactor * torch.sin(freq * d_scaled) / (d + 1e-6)
 
@torch.jit.script
def compute_envelope_math(d: torch.Tensor, r_cut: float) -> torch.Tensor:
    x = d / r_cut
    x = torch.clamp(x, min=0.0, max=1.0)
    return 1.0 - 10.0 * x**3 + 15.0 * x**4 - 6.0 * x**5

@torch.jit.script
def compute_l2_basis(rbf_feat: torch.Tensor, r_hat: torch.Tensor) -> torch.Tensor:
    outer = r_hat.unsqueeze(2) * r_hat.unsqueeze(1) 
    eye = torch.eye(3, dtype=r_hat.dtype, device=r_hat.device).unsqueeze(0)
    trace_less = outer - (1.0/3.0) * eye
    return rbf_feat.unsqueeze(1).unsqueeze(1) * trace_less.unsqueeze(-1)

@torch.jit.script
def compute_invariants(den0: Optional[torch.Tensor], 
                       den1: Optional[torch.Tensor], 
                       den2: Optional[torch.Tensor]) -> torch.Tensor:
    # âœ… ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†ç±»å‹æ ‡æ³¨
    invariants: List[torch.Tensor] = []
    
    if den0 is not None:
        invariants.append(den0)
        
    if den1 is not None:
        sq_sum = torch.sum(den1.pow(2), dim=1) 
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    if den2 is not None:
        sq_sum = torch.sum(den2.pow(2), dim=(1, 2))
        norm = torch.sqrt(sq_sum + 1e-8)
        invariants.append(norm)
        
    if len(invariants) > 0:
        return torch.cat(invariants, dim=-1)
    else:
        # è¿”å›ç©º Tensor (æ³¨æ„å¤„ç† device é—®é¢˜ï¼Œæœ€å¥½ç”±å¤–éƒ¨ä¿è¯ invariants ä¸ä¸ºç©º)
        return torch.zeros(0) 

@torch.jit.script
def compute_gating_projections(h_node1: torch.Tensor, 
                               r_hat: torch.Tensor, 
                               scalar_basis: torch.Tensor,
                               src: torch.Tensor, 
                               dst: torch.Tensor) -> torch.Tensor:
    r_hat_uns = r_hat.unsqueeze(-1)
    p_src = torch.sum(h_node1[src] * r_hat_uns, dim=1)
    p_dst = torch.sum(h_node1[dst] * r_hat_uns, dim=1)
    return torch.cat([scalar_basis, p_src, p_dst], dim=-1)


# ==========================================
# ğŸ§© æ¨¡å—å®šä¹‰ (æ™®é€š nn.Module åŒº)
# ==========================================

class BesselBasis(nn.Module): 
    def __init__(self, r_max: float, num_basis: int = 8):
        super().__init__()
        self.r_max = float(r_max)
        self.num_basis = int(num_basis)
        self.register_buffer("freq", torch.arange(1, num_basis + 1).float() * np.pi)

    def forward(self, d: torch.Tensor) -> torch.Tensor:
        return compute_bessel_math(d, self.r_max, self.freq)

class PolynomialEnvelope(nn.Module):
    def __init__(self, r_cut: float, p: int = 5):
        super().__init__()
        self.r_cutoff = float(r_cut)
        self.p = int(p)
    
    def forward(self, d_ij: torch.Tensor) -> torch.Tensor:
        return compute_envelope_math(d_ij, self.r_cutoff)

class GeometricBasis(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.rbf = BesselBasis(config.cutoff, config.num_rbf)
        self.envelope = PolynomialEnvelope(r_cut=config.cutoff)
        self.rbf_mlp = nn.Sequential(
            nn.Linear(config.num_rbf, config.hidden_dim),
            nn.SiLU(),
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )

    def forward(self, vec_ij, d_ij):
        raw_rbf = self.rbf_mlp(self.rbf(d_ij.unsqueeze(-1)))
        env = self.envelope(d_ij)
        rbf_feat = raw_rbf * env.unsqueeze(-1)

        # âš ï¸ r_hat è®¡ç®—å¿…é¡»åœ¨ Python å±‚ä¿ç•™ï¼Œç¡®ä¿æ¢¯åº¦ä¼ å¯¼
        r_hat = vec_ij / (d_ij.unsqueeze(-1) + 1e-6)
        
        basis = {}
        basis[0] = rbf_feat
        
        if self.cfg.use_L1 or self.cfg.use_L2:
            basis[1] = rbf_feat.unsqueeze(1) * r_hat.unsqueeze(-1)
            
        if self.cfg.use_L2:
            basis[2] = compute_l2_basis(rbf_feat, r_hat)
            
        return basis, r_hat

class LeibnizCoupling(nn.Module): 
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        self.path_weights = nn.ModuleDict()
        
        for path_key, active in config.active_paths.items():
            if not active: continue
            l_in, l_edge, l_out, _ = path_key
            if (l_in == 2 or l_edge == 2 or l_out == 2) and not config.use_L2: continue
            if (l_in == 1 or l_edge == 1 or l_out == 1) and not config.use_L1: continue
                
            name = f"{l_in}_{l_edge}_{l_out}_{path_key[3]}"
            self.path_weights[name] = nn.Linear(self.F, self.F, bias=False)

        self.inv_sqrt_f = self.F ** -0.5

    def forward(self, h_nodes: Dict[int, torch.Tensor], basis_edges: Dict[int, torch.Tensor], edge_index):
        src, _ = edge_index
        messages: Dict[int, List[torch.Tensor]] = {0: [], 1: [], 2: []}
        
        for path_key, active in self.cfg.active_paths.items():
            if not active: continue
            l_in, l_edge, l_out, op_type = path_key
            
            if basis_edges.get(l_edge) is None: continue
            
            layer_name = f"{l_in}_{l_edge}_{l_out}_{op_type}"
            if layer_name not in self.path_weights: continue
            
            if h_nodes.get(l_in) is None: continue 
            else: inp = h_nodes[l_in]
            
            h_src = inp[src]
            h_trans = self.path_weights[layer_name](h_src)
            geom = basis_edges[l_edge]
            res = None
            
            # --- Operation Logic ---
            if op_type == 'prod':
                if l_in == 0 and l_edge == 0: res = h_trans * geom
                elif l_in == 0 and l_edge == 1: res = h_trans.unsqueeze(1) * geom
                elif l_in == 0 and l_edge == 2: res = h_trans.unsqueeze(1).unsqueeze(1) * geom
                elif l_in == 1 and l_edge == 0: res = h_trans * geom.unsqueeze(1)
                elif l_in == 2 and l_edge == 0: res = h_trans * geom.unsqueeze(1).unsqueeze(1)
            elif op_type == 'dot':
                res = torch.sum(h_trans * geom, dim=1)
            elif op_type == 'cross':
                g = geom
                if g.dim() == 2: g = g.unsqueeze(-1)
                res = torch.linalg.cross(h_trans, g, dim=1)
            elif op_type == 'outer':
                outer = h_trans.unsqueeze(2) * geom.unsqueeze(1)
                trace = torch.einsum('eiif->ef', outer)
                eye = torch.eye(3, device=outer.device).view(1, 3, 3, 1)
                res = outer - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye
            elif op_type == 'mat_vec':
                res = torch.einsum('eijf, ejf -> eif', h_trans, geom)
            elif op_type == 'vec_mat':
                res = torch.einsum('eif, eijf -> ejf', h_trans, geom)
            elif op_type == 'double_dot':
                res = torch.sum(h_trans * geom, dim=(1, 2))
            elif op_type == 'mat_mul_sym':
                raw = torch.einsum('eikf, ekjf -> eijf', h_trans, geom)
                sym = 0.5 * (raw + raw.transpose(1, 2))
                trace = torch.einsum('eiif->ef', sym)
                eye = torch.eye(3, device=sym.device).view(1, 3, 3, 1)
                res = sym - (1.0/3.0) * trace.unsqueeze(1).unsqueeze(1) * eye

            if res is not None:
                messages[l_out].append(res * self.inv_sqrt_f)
                
        final_msgs: Dict[int, Optional[torch.Tensor]] = {}
        for l in [0, 1, 2]:
            final_msgs[l] = sum(messages[l]) if len(messages[l]) > 0 else None
        return final_msgs

class PhysicsGating(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        self.W_query = nn.Linear(self.F, self.F, bias=False)
        self.W_key = nn.Linear(self.F, self.F, bias=False)
        
        self.phys_bias_mlp = nn.Sequential(
            nn.Linear(3 * self.F, self.F), 
            nn.SiLU(),            
            nn.Linear(self.F, 3 * self.F) 
        )
        self.channel_mixer = nn.Linear(self.F, 3 * self.F, bias=False)
        self.gate_scale = nn.Parameter(torch.ones(1) * 2.0)

    def forward(self, msgs, h_node0, scalar_basis, r_hat, h_node1, edge_index, capture_weights=False):
        if not self.cfg.use_gating: return msgs
        
        src, dst = edge_index
        
        if h_node1 is not None:
            phys_input = compute_gating_projections(h_node1, r_hat, scalar_basis, src, dst)
            split_idx = scalar_basis.shape[-1]
            p_ij = phys_input[:, split_idx:]        
        else:
            p_ij = torch.zeros((scalar_basis.shape[0], 2 * self.F), device=scalar_basis.device)
            phys_input = torch.cat([scalar_basis, p_ij], dim=-1)

        q = self.W_query(h_node0[dst]) 
        k = self.W_key(h_node0[src])   
        chem_score = q * k             
        chem_logits = self.channel_mixer(chem_score)
        phys_logits = self.phys_bias_mlp(phys_input)
        
        raw_gates = chem_logits + phys_logits
        gates = torch.sigmoid(raw_gates) * self.gate_scale
        
        if capture_weights: self.scalar_basis_captured = scalar_basis.detach()
        if capture_weights: self.p_ij_captured = p_ij.detach()
        if capture_weights: self.chem_logits_captured = chem_logits.detach()
        if capture_weights: self.phys_logits_captured = phys_logits.detach()

        g_list = torch.split(gates, self.F, dim=-1)
        g0, g1, g2 = [g.contiguous() for g in g_list]

        if capture_weights: self.g0_captured = g0.detach()
        if capture_weights: self.g1_captured = g1.detach()
        if capture_weights: self.g2_captured = g2.detach()
        
        out_msgs: Dict[int, torch.Tensor] = {}
        if 0 in msgs and msgs[0] is not None: out_msgs[0] = msgs[0] * g0
        if 1 in msgs and msgs[1] is not None: out_msgs[1] = msgs[1] * g1.unsqueeze(1)
        if 2 in msgs and msgs[2] is not None: out_msgs[2] = msgs[2] * g2.unsqueeze(1).unsqueeze(1)
            
        return out_msgs

class CartesianDensityBlock(nn.Module):
    def __init__(self, config: HTGPConfig):
        super().__init__()
        self.F = config.hidden_dim
        self.cfg = config
        
        in_dim = 0
        if config.use_L0: in_dim += self.F
        if config.use_L1: in_dim += self.F
        if config.use_L2: in_dim += self.F 
        
        self.scalar_update_mlp = nn.Sequential(
            nn.Linear(in_dim, self.F),
            nn.SiLU(),
            nn.Linear(self.F, self.F)
        )

        if config.use_L1: self.L1_linear = nn.Linear(self.F, self.F, bias=False)
        if config.use_L2: self.L2_linear = nn.Linear(self.F, self.F, bias=False)

        scale_out_dim = 0
        if config.use_L1: scale_out_dim += self.F
        if config.use_L2: scale_out_dim += self.F
        
        if scale_out_dim > 0:
            self.scale_mlp = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, scale_out_dim)
            )
        else:
            self.scale_mlp = None
 
        self.inv_sqrt_deg = 1.0 / (config.avg_neighborhood ** 0.5)

    def forward(self, msgs: Dict[int, torch.Tensor], index: torch.Tensor, num_nodes: int):
        # 1. å¯†åº¦èšåˆ
        # âœ… ä¿®æ­£ï¼šæ ‡å‡†ç±»å‹æ ‡æ³¨ï¼Œæ˜ç¡® None
        densities: Dict[int, Optional[torch.Tensor]] = {}
        densities[0], densities[1], densities[2] = None, None, None

        for l in [0, 1, 2]:
            if l in msgs and msgs[l] is not None:
                agg = scatter_add(msgs[l], index, dim=0, dim_size=num_nodes)
                densities[l] = agg * self.inv_sqrt_deg 
            else:
                densities[l] = None

        # 2. æå–ä¸å˜é‡
        concat = compute_invariants(densities[0], densities[1], densities[2])

        # 3. æ ‡é‡æ›´æ–°
        # âœ… ä¿®æ­£ï¼šä½¿ç”¨ index.device é¿å…æ­§ä¹‰æŠ¥é”™
        if concat.numel() > 0:
            delta_h0 = self.scalar_update_mlp(concat)
        else:
            delta_h0 = torch.zeros((num_nodes, self.F), device=index.device)

        # 4. çŸ¢é‡æ›´æ–°
        delta_h1 = None
        delta_h2 = None

        if self.scale_mlp is not None:
            scales = self.scale_mlp(delta_h0)
            curr_dim = 0
            
            if self.cfg.use_L1 and densities[1] is not None:
                alpha1 = scales[:, curr_dim : curr_dim + self.F] 
                h1_mixed = self.L1_linear(densities[1])
                delta_h1 = h1_mixed * alpha1.unsqueeze(1)
                curr_dim += self.F
                
            if self.cfg.use_L2 and densities[2] is not None:
                alpha2 = scales[:, curr_dim : curr_dim + self.F]
                h2_mixed = self.L2_linear(densities[2])
                delta_h2 = h2_mixed * alpha2.unsqueeze(1).unsqueeze(1)

        return delta_h0, delta_h1, delta_h2

# ==========================================
# 6. é•¿ç¨‹åœº (Latent Long Range) - Ablation Ready
# ==========================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple

# ç‰©ç†å¸¸æ•° (eV * A)
# k_e = 1 / (4 * pi * epsilon_0)
KE_CONST = 14.3996 

# ==============================================================================
# ğŸ”¥ æ ¸å¿ƒæ•°å­¦å†…æ ¸ (JIT Script)
# è¿™äº›å‡½æ•°ä¼šè¢«ç¼–è¯‘ä¸º C++ è¿è¡Œæ—¶ï¼Œæå¤§æå‡ For å¾ªç¯å’ŒçŸ©é˜µæ“ä½œçš„é€Ÿåº¦
# ==============================================================================

@torch.jit.script
def compute_direct_electrostatics_jit(
    q: torch.Tensor, 
    dist: torch.Tensor, 
    batch_mask: torch.Tensor,
    sigma: float
) -> torch.Tensor:
    """
    ã€å®ç©ºé—´æ±‚å’Œã€‘é€‚ç”¨äºæœ‰é™ä½“ç³»(Cluster)æˆ–å‘¨æœŸæ€§ä½“ç³»çš„çŸ­ç¨‹ä¿®æ­£ã€‚
    
    å¯¹åº”å…¬å¼: E = 1/2 * k_e * sum_{i,j} (q_i*q_j)/r * erf(r / (sqrt(2)*sigma))
    ç‰©ç†æ„ä¹‰: è®¡ç®—ä¸¤ä¸ªå®½åº¦ä¸º sigma çš„é«˜æ–¯ç”µè·çƒä¹‹é—´çš„é™ç”µç›¸äº’ä½œç”¨ã€‚
    """
    # 1. ç”µè·ä¹˜ç§¯çŸ©é˜µ q_i * q_j
    qq = q @ q.t()  # (N, N)
    
    # 2. å€’è·ç¦» 1/r (åŠ  epsilon é˜²æ­¢é™¤ 0)
    inv_dist = 1.0 / (dist + 1e-8)
    
    # 3. å±è”½å› å­ (Screening Factor): erf(r / (sqrt(2) * sigma))
    # ä½œç”¨: 
    #   r -> inf: erf -> 1, æ¢å¤æ ‡å‡†åº“ä»‘å®šå¾‹ 1/r
    #   r -> 0:   erf/r -> const, æ¶ˆé™¤ r=0 å¤„çš„æ— ç©·å¤§å¥‡ç‚¹
    #   çŸ­ç¨‹ç¼ºå¤±çš„ (1-erf)/r éƒ¨åˆ†ç”± GNN è´Ÿè´£æ‹Ÿåˆ (erfcéƒ¨åˆ†)
    sqrt2 = 1.41421356
    scaled_r = dist / (sqrt2 * sigma)
    shielding = torch.erf(scaled_r)
    
    # 4. ç»„åˆèƒ½é‡
    # E_matrix = (q_i * q_j / r) * erf(...)
    E_matrix = qq * inv_dist * shielding
    
    # 5. æ±‚å’Œ
    # batch_mask: ç¡®ä¿ä¸è®¡ç®—ä¸åŒåˆ†å­é—´çš„åŸå­
    # diag_mask(å¤–éƒ¨å¤„ç†): ç¡®ä¿ä¸è®¡ç®— i=j
    E_sum = torch.sum(E_matrix * batch_mask)
    
    # ä¹˜ä»¥ 0.5 (æ¶ˆé™¤åŒé‡è®¡æ•° i-j å’Œ j-i) å’Œ åº“ä»‘å¸¸æ•°
    return 0.5 * KE_CONST * E_sum

@torch.jit.script
def compute_bj_damping_vdw_jit(
    c6: torch.Tensor,
    r_vdw: torch.Tensor,
    dist_sq: torch.Tensor,
    batch_mask: torch.Tensor
) -> torch.Tensor:
    """
    ã€èŒƒå¾·ååŠ›ã€‘Becke-Johnson (BJ) é˜»å°¼å½¢å¼ã€‚
    
    å¯¹åº”å…¬å¼: E = - sum C6_ij / (r^6 + f(R_vdw)^6)
    ç‰©ç†æ„ä¹‰: æ¨¡æ‹Ÿä¼¦æ•¦è‰²æ•£åŠ›ï¼ŒåŒæ—¶é˜²æ­¢ r->0 æ—¶èƒ½é‡å‘æ•£ã€‚
    """
    # ç»„åˆè§„åˆ™: å‡ ä½•å¹³å‡
    c6_ij = torch.sqrt(c6 @ c6.t())
    rvdw_ij = torch.sqrt(r_vdw @ r_vdw.t())
    
    # è®¡ç®— r^6
    dist6 = dist_sq ** 3
    
    # BJ é˜»å°¼åˆ†æ¯: r^6 + R_vdw^6
    damping = dist6 + (rvdw_ij ** 6)
    
    # èƒ½é‡æ±‚å’Œ (è´Ÿå·è¡¨ç¤ºå¸å¼•)
    E_matrix = - (c6_ij / (damping + 1e-8)) * batch_mask
    return 0.5 * torch.sum(E_matrix)

@torch.jit.script
def generate_k_template(k_cutoff: float, device: torch.device) -> torch.Tensor:
    """
    ç”Ÿæˆä¸€ä¸ªé€šç”¨çš„æ•´æ•°ç½‘æ ¼ (n1, n2, n3)ã€‚
    ç”¨äºæ„å»º K å‘é‡ã€‚
    """
    # ä¼°è®¡éœ€è¦çš„æ•´æ•°èŒƒå›´ã€‚å¯¹äºå¤§å¤šæ•°æ™¶èƒï¼Œ[-10, 10] è¶³å¤Ÿè¦†ç›– k_cutoff < 6.0
    # å®é™…åº”ç”¨ä¸­å¯ä»¥åŠ¨æ€è®¡ç®—ï¼Œè¿™é‡Œä¸ºäº† JIT æ•ˆç‡è®¾ä¸ºå›ºå®šèŒƒå›´
    n_max = 8 
    rng = torch.arange(-n_max, n_max + 1, device=device, dtype=torch.float32)
    n1, n2, n3 = torch.meshgrid(rng, rng, rng, indexing='ij')
    
    # (M, 3) æ•´æ•°å‘é‡
    n = torch.stack([n1.flatten(), n2.flatten(), n3.flatten()], dim=1)
    
    # å‰”é™¤ (0,0,0)ï¼Œå› ä¸º Ewald æ±‚å’Œä¸åŒ…å« k=0 é¡¹ (èƒŒæ™¯ç”µè·ä¸­å’Œ)
    n_sq = torch.sum(n**2, dim=1)
    mask = n_sq > 0
    return n[mask]

@torch.jit.script
def compute_ewald_kspace_jit(
    q: torch.Tensor,
    pos: torch.Tensor,
    batch: torch.Tensor,
    cell: torch.Tensor,
    n_grid: torch.Tensor,
    sigma: float,
    k_cutoff: float,
    num_graphs: int
) -> torch.Tensor:
    """
    ã€å€’ç©ºé—´ Ewald æ±‚å’Œã€‘é€‚ç”¨äºå‘¨æœŸæ€§ä½“ç³» (PBC)ã€‚
    
    å¯¹åº” JCTC è®ºæ–‡ Eq. (1) å’Œ (2):
    E_recip = 1/(2*eps*V) * sum_k [ exp(-sigma^2 k^2 / 2) / k^2 * |S(k)|^2 ]
    """
    # 1. æ„å»ºå€’æ ¼å­å‘é‡ B = 2*pi * (A^-1)^T
    # cell: (B, 3, 3) -> recip_cell: (B, 3, 3)
    recip_cell = 2 * math.pi * torch.inverse(cell).transpose(1, 2)
    
    # 2. ç”Ÿæˆç‰©ç† K å‘é‡: K = n @ B
    # n_grid: (M, 3)
    # recip_cell: (B, 3, 3)
    # ç»“æœ k_vecs: (B, M, 3) - æ¯ä¸ª batch æœ‰è‡ªå·±çš„ä¸€å¥— K å‘é‡
    k_vecs = torch.matmul(n_grid.unsqueeze(0), recip_cell) 
    
    # 3. è¿‡æ»¤ K å‘é‡ (|k| < k_cutoff)
    # ä¸ºäº†ä¿æŒ batch ç»´åº¦ä¸€è‡´ï¼Œè¿™é‡Œé‡‡ç”¨ soft mask (ä¹˜ä»¥0) æˆ–è€…åªä¿ç•™éƒ½åœ¨èŒƒå›´å†…çš„
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è®¡ç®—æ‰€æœ‰ n_grid å¯¹åº”çš„ kï¼Œç„¶åé€šè¿‡æƒé‡è¡°å‡è‡ªç„¶è¿‡æ»¤å¤§çš„ k
    k_sq = torch.sum(k_vecs**2, dim=-1) # (B, M)
    
    # 4. è®¡ç®—ç»“æ„å› å­ S(k) = sum_j q_j * exp(i * k * r_j)
    # å°† k_vecs æ˜ å°„åˆ°æ¯ä¸ªåŸå­: (B, M, 3) -> (N, M, 3)
    k_vecs_expanded = k_vecs[batch] 
    
    # è®¡ç®—ç›¸è§’ k * r: (N, M, 3) * (N, 1, 3) -> sum -> (N, M)
    kr = torch.sum(k_vecs_expanded * pos.unsqueeze(1), dim=-1)
    
    # æ¬§æ‹‰å…¬å¼
    cos_kr = torch.cos(kr)
    sin_kr = torch.sin(kr)
    
    # æŒ‰ Batch èšåˆæ±‚å’Œ S(k)
    # S_real[b, k] = sum_{i in b} q_i * cos(k*r_i)
    # è¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ scatter æ“ä½œ
    Sk_real = torch.zeros(num_graphs, n_grid.size(0), device=q.device, dtype=q.dtype)
    Sk_imag = torch.zeros(num_graphs, n_grid.size(0), device=q.device, dtype=q.dtype)
    
    Sk_real.index_add_(0, batch, q * cos_kr)
    Sk_imag.index_add_(0, batch, q * sin_kr)
    
    # æ¨¡æ–¹ |S(k)|^2: (B, M)
    Sk_sq = Sk_real**2 + Sk_imag**2
    
    # 5. è®¡ç®—èƒ½é‡é¡¹
    # prefactor = exp(-sigma^2 * k^2 / 2) / k^2
    # å¯¹äº k=0 æˆ–æå°å€¼ï¼Œexp/k^2 ä¼šçˆ†ç‚¸ï¼Œä½†æˆ‘ä»¬åœ¨ generate_k_template å·²ç»å‰”é™¤äº† n=0
    prefactor = torch.exp(-0.5 * sigma**2 * k_sq) / (k_sq + 1e-12)
    
    # ç¡¬æˆªæ–­: å¦‚æœ k^2 å¾ˆå¤§ï¼Œprefactor æå°ï¼Œæ•°å€¼ä¸Šå®‰å…¨
    # å¦‚æœè¦ä¸¥æ ¼æˆªæ–­:
    # mask = k_sq < k_cutoff**2
    # prefactor = prefactor * mask.float()
    
    # å€’ç©ºé—´èƒ½é‡: Sum_k (prefactor * Sk_sq) -> (B,)
    E_recip_raw = torch.sum(prefactor * Sk_sq, dim=1)
    
    # 6. ç³»æ•°ä¿®æ­£
    # ç³»æ•° = 1 / (2 * epsilon_0 * V)
    # æˆ‘ä»¬æœ‰ KE_CONST = 1 / (4 * pi * epsilon_0)
    # æ‰€ä»¥ 1 / (2 * epsilon_0) = 2 * pi * KE_CONST
    vol = torch.abs(torch.det(cell)) # (B,)
    coeff = (2 * math.pi * KE_CONST) / vol
    
    E_recip = coeff * E_recip_raw
    
    # 7. å‡å»è‡ªèƒ½ (Self Energy Correction)
    # å€’ç©ºé—´æ±‚å’ŒåŒ…å«äº† i=i çš„é«˜æ–¯è‡ªä½œç”¨ï¼Œå¿…é¡»å‡å»
    # E_self = k_e * (1 / (sqrt(2*pi)*sigma)) * sum(q^2)
    q_sq = q**2
    q_sq_sum = torch.zeros(num_graphs, 1, device=q.device, dtype=q.dtype)
    q_sq_sum.index_add_(0, batch, q_sq)
    q_sq_sum = q_sq_sum.squeeze(-1)
    
    self_prefactor = 1.0 / (math.sqrt(2.0 * math.pi) * sigma)
    E_self = KE_CONST * self_prefactor * q_sq_sum
    
    # æ€»é•¿ç¨‹èƒ½é‡ (GNN è´Ÿè´£å®ç©ºé—´ erfc éƒ¨åˆ†)
    return E_recip - E_self


class LatentLongRange(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.cfg = config
        self.F = config.hidden_dim
        
        # --- 1. ç‰©ç†å‚æ•°é¢„æµ‹å±‚ ---
        if config.use_charge:
            # è¾“å…¥: æ ‡é‡ç‰¹å¾ h0 -> è¾“å‡º: ç”µè· q
            self.q_proj = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, 1, bias=False) # æ— åç½®ï¼Œç‰¹å¾ä¸º0åˆ™ç”µè·ä¸º0
            )
            # å¯å­¦ä¹ çš„é«˜æ–¯å®½åº¦ sigma (åˆå§‹å€¼ 1.0 A)
            # å†³å®šäº†å®ç©ºé—´å’Œå€’ç©ºé—´çš„åˆ†ç•Œï¼Œä»¥åŠ GNN éœ€è¦æ‹Ÿåˆçš„çŸ­ç¨‹èŒƒå›´
            self.sigma = nn.Parameter(torch.tensor(1.0))

        if config.use_vdw:
            # è¾“å…¥: æ ‡é‡ç‰¹å¾ h0 -> è¾“å‡º: C6ç³»æ•°, èŒƒå¾·ååŠå¾„ R_vdw
            self.vdw_proj = nn.Sequential(
                nn.Linear(self.F, self.F),
                nn.SiLU(),
                nn.Linear(self.F, 2)
            )

        if config.use_dipole:
            # è¾“å…¥: çŸ¢é‡ç‰¹å¾ h1 -> è¾“å‡º: å¶æçŸ© mu
            self.mu_proj = nn.Linear(self.F, 1, bias=False)

        # ç¼“å­˜: æ•´æ•°ç½‘æ ¼æ¨¡æ¿ (é¿å…æ¯æ¬¡ç”Ÿæˆ)
        self.register_buffer('n_grid_cache', None)

    def forward(self, h0, h1, pos, batch, cell: Optional[torch.Tensor] = None):
        """
        å‰å‘ä¼ æ’­
        Args:
            h0: (N, F) æ ‡é‡ç‰¹å¾
            h1: (N, 3, F) çŸ¢é‡ç‰¹å¾
            pos: (N, 3) åŸå­åæ ‡
            batch: (N,) æ‰¹æ¬¡ç´¢å¼•
            cell: (B, 3, 3) æ™¶èƒçŸ©é˜µ. å¦‚æœä¸º None, åˆ™è®¤ä¸ºæ˜¯æœ‰é™ä½“ç³»(Cluster)ã€‚
        """
        total_energy = 0.0
        num_graphs = int(batch.max()) + 1
        
        # ----------------------------------------------------
        # 1. é¢„æµ‹ç‰©ç†å‚æ•° (Physics Parameters)
        # ----------------------------------------------------
        q = None
        c6, r_vdw = None, None
        
        if self.cfg.use_charge:
            q = self.q_proj(h0) # (N, 1)
            
            # [ç‰©ç†çº¦æŸ] ç”µè·ä¸­æ€§åŒ– (Charge Neutrality)
            # ç®—å‡ºæ¯ä¸ª graph çš„å¹³å‡ç”µè·ï¼Œç„¶åå‡å»ï¼Œç¡®ä¿ sum(q) = 0
            q_sum = torch.zeros(num_graphs, 1, device=q.device, dtype=q.dtype)
            q_sum.index_add_(0, batch, q)
            
            counts = torch.zeros(num_graphs, 1, device=q.device, dtype=q.dtype)
            ones = torch.ones_like(q)
            counts.index_add_(0, batch, ones)
            
            q_mean = q_sum / counts.clamp(min=1.0)
            q = q - q_mean[batch] # å¹¿æ’­å‡å»å‡å€¼

        if self.cfg.use_vdw:
            vdw_params = self.vdw_proj(h0)
            # ç‰©ç†é‡å¿…é¡»ä¸ºæ­£ï¼Œä½¿ç”¨ Softplus
            c6 = F.softplus(vdw_params[:, 0:1])
            r_vdw = F.softplus(vdw_params[:, 1:2])

        # ----------------------------------------------------
        # 2. åˆ†æ”¯ A: å‘¨æœŸæ€§ä½“ç³» (PBC) -> Ewald K-Space
        # ----------------------------------------------------
        if cell is not None:
            # ç¡®ä¿ cell å½¢çŠ¶æ­£ç¡® (B, 3, 3)
            if cell.dim() == 2: cell = cell.unsqueeze(0)
            if cell.shape[0] != num_graphs: 
                cell = cell.expand(num_graphs, -1, -1)

            # [é™ç”µåŠ›]
            if self.cfg.use_charge and q is not None:
                # æ‡’åŠ è½½ç”Ÿæˆæ•´æ•°ç½‘æ ¼æ¨¡æ¿
                if self.n_grid_cache is None:
                    self.n_grid_cache = generate_k_template(k_cutoff=6.0, device=pos.device)
                
                # è®¡ç®—å€’ç©ºé—´èƒ½é‡ + å‡å»è‡ªèƒ½
                # æ³¨æ„: å®ç©ºé—´éƒ¨åˆ† (erfc) ç”± GNN æ‹Ÿåˆ
                e_elec_batch = compute_ewald_kspace_jit(
                    q, pos, batch, cell, self.n_grid_cache, 
                    self.sigma, k_cutoff=6.0, num_graphs=num_graphs
                )
                total_energy += torch.sum(e_elec_batch)

            # [èŒƒå¾·å]
            # PBC ä¸‹ VdW é•¿ç¨‹éƒ¨åˆ†(> cutoff) è´¡çŒ®å¾ˆå°ï¼Œé€šå¸¸ç”± GNN éšå¼å­¦ä¹ 
            # æˆ–è€…ä½¿ç”¨ç®€å•çš„è§£æç§¯åˆ†ä¿®æ­£ (Tail Correction)ã€‚
            # ä¸ºäº†æ•ˆç‡ï¼Œè¿™é‡Œæš‚ä¸æ˜¾å¼è®¡ç®— PBC VdW é•¿ç¨‹ã€‚
            pass

        # ----------------------------------------------------
        # 3. åˆ†æ”¯ B: æœ‰é™ä½“ç³» (Cluster) -> Direct Sum
        # ----------------------------------------------------
        else:
            # è®¡ç®—å…¨è¿æ¥è·ç¦»çŸ©é˜µ (O(N^2))
            # ä¼˜åŒ–: ä»…è®¡ç®—åæ ‡å·®å’Œè·ç¦»ï¼Œé¿å…ä¸å¿…è¦çš„ä¸­é—´å˜é‡
            diff = pos.unsqueeze(1) - pos.unsqueeze(0)
            dist_sq = torch.sum(diff**2, dim=-1)
            dist = torch.sqrt(dist_sq + 1e-8)
            
            # Mask: æ’é™¤ä¸åŒ batch å’Œ è‡ªç›¸äº’ä½œç”¨
            batch_mask = (batch.unsqueeze(1) == batch.unsqueeze(0))
            diag_mask = torch.eye(pos.size(0), device=pos.device, dtype=torch.bool)
            valid_mask = batch_mask & (~diag_mask)
            mask_float = valid_mask.float() # JIT éœ€è¦ float

            # [é™ç”µåŠ›] Direct Sum with erf Screening
            if self.cfg.use_charge and q is not None:
                e_elec = compute_direct_electrostatics_jit(
                    q, dist, mask_float, self.sigma
                )
                total_energy += e_elec
            
            # [èŒƒå¾·å] Direct Sum with BJ Damping
            if self.cfg.use_vdw and c6 is not None:
                e_vdw = compute_bj_damping_vdw_jit(
                    c6, r_vdw, dist_sq, mask_float
                )
                total_energy += e_vdw
                
            # [å¶æçŸ©] (å¯é€‰)
            if self.cfg.use_dipole and h1 is not None:
                # è¿™é‡Œçš„é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œä¸ºäº†ä»£ç æ¸…æ™°åº¦æœªæ”¾å…¥ JITï¼Œ
                # å¦‚æœéœ€è¦å¯ä»¥å‚è€ƒä¹‹å‰çš„å›å¤å°†å…¶ JIT åŒ–
                mu = self.mu_proj(h1).squeeze(-1)
                # ... (åŒä¹‹å‰çš„å®ç°)

        return total_energy * self.cfg.long_range_scale