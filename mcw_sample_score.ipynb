{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290134ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from typing import Tuple, Dict, Callable, TypeVar, Optional, Any\n",
    "from typing_extensions import Protocol, runtime_checkable\n",
    "R = TypeVar(\"R\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "@runtime_checkable\n",
    "class BatchedData(Protocol):\n",
    "    def replace(self: T, **vals: torch.Tensor) -> T:\n",
    "        \"\"\"Return a copy of self with some fields replaced with new values.\"\"\"\n",
    "\n",
    "    def get_batch_idx(self, field_name: str) -> torch.LongTensor | None:\n",
    "        \"\"\"Get the batch index (i.e., which row belongs to which sample) for a given field.\n",
    "        For 'dense' type data, where every sample has the same shape and the first dimension is the\n",
    "        batch dimension, this method should return None. Mathematically,\n",
    "        returning None will be treated the same as returning a tensor [0, 1, 2, ..., batch_size - 1]\n",
    "        but I expect memory access in other functions to be more efficient if you return None.\n",
    "        \"\"\"\n",
    "\n",
    "    def get_batch_size(self) -> int:\n",
    "        \"\"\"Get the batch size.\"\"\"\n",
    "\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"Get the device of the batch.\"\"\"\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> torch.Tensor:\n",
    "        \"\"\"Get a field from the batch.\"\"\"\n",
    "\n",
    "    def to(self: T, device: torch.device) -> T:\n",
    "        \"\"\"Move the batch to a given device.\"\"\"\n",
    "\n",
    "    def clone(self: T) -> T:\n",
    "        \"\"\"Return a copy with all the tensors cloned.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diffusable = TypeVar(\"Diffusable\", bound=BatchedData)\n",
    "SampleAndMean = Tuple[Diffusable, Diffusable]\n",
    "SampleAndMeanAndMaybeRecords = Tuple[Diffusable, Diffusable, list[Diffusable] | None]\n",
    "SampleAndMeanAndRecords = Tuple[Diffusable, Diffusable, list[Diffusable]]\n",
    "B = Optional[torch.LongTensor]\n",
    "\n",
    "def apply(fns: Dict[str, Callable[..., R]], broadcast, **kwargs) -> Dict[str, R]:\n",
    "    \"\"\"Apply different function with different argument values to each field.\n",
    "    fns: dict of the form {field_name: function_to_apply}\n",
    "    broadcast: arguments that are identical for every field_name\n",
    "    kwargs: dict of the form {argument_name: {field_name: argument_value}}\n",
    "    \"\"\"\n",
    "    return {\n",
    "        field_name: fn(\n",
    "            **{k: v[field_name] for k, v in kwargs.items() if field_name in v},\n",
    "            **(broadcast or dict()),\n",
    "        )\n",
    "        for field_name, fn in fns.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def broadcast_like(x, like):\n",
    "    \"\"\"\n",
    "    add broadcast dimensions to x so that it can be broadcast over ``like``\n",
    "    \"\"\"\n",
    "    if like is None:\n",
    "        return x\n",
    "    return x[(...,) + (None,) * (like.ndim - x.ndim)]\n",
    "\n",
    "\n",
    "def maybe_expand(x: torch.Tensor, batch: B, like: torch.Tensor = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x: shape (batch_size, ...)\n",
    "        batch: shape (num_thingies,) with integer entries in the range [0, batch_size), indicating which sample each thingy belongs to\n",
    "        like: shape x.shape + potential additional dimensions\n",
    "    Returns:\n",
    "        expanded x with shape (num_thingies,), or if given like.shape, containing value of x for each thingy.\n",
    "        If `batch` is None, just returns `x` unmodified, to avoid pointless work if you have exactly one thingy per sample.\n",
    "    \"\"\"\n",
    "    x = broadcast_like(x, like)\n",
    "    if batch is None:\n",
    "        return x\n",
    "    else:\n",
    "        if x.shape[0] == batch.shape[0]:\n",
    "            print(\n",
    "                \"Warning: batch shape is == x shape, are you trying to expand something that is already expanded?\"\n",
    "            )\n",
    "        return x[batch]\n",
    "    \n",
    "\n",
    "def wrap(x, wrapping_boundary):\n",
    "    return torch.remainder(\n",
    "        x, wrapping_boundary\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Mapping, Union\n",
    "\n",
    "\n",
    "class ModelTarget(Enum):\n",
    "    \"\"\"Specifies what the score model is trained to predict.\n",
    "    Only relevant for fields that are corrupted with an SDE.\"\"\"\n",
    "\n",
    "    score_times_std = \"score_times_std\"  # Predict -z where z is gaussian noise with unit variance used to corrupt the data\n",
    "    logits = \"logits\"  # Predict logits for a categorical variable\n",
    "\n",
    "model_target = \"score_times_std\"  # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780adc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_out_to_score(\n",
    "    *,\n",
    "    model_target: ModelTarget,\n",
    "    sde: SDE,\n",
    "    model_out: torch.Tensor,\n",
    "    batch_idx: torch.LongTensor,\n",
    "    t: torch.Tensor,\n",
    "    batch: Any\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a model output to a score, according to the specified model_target.\n",
    "\n",
    "    model_target: says what the model predicts.\n",
    "        For example, in RFDiffusion the model predicts clean coordinates;\n",
    "        in EDM the model predicts the raw noise.\n",
    "    sde: corruption process\n",
    "    model_out: model output\n",
    "    batch_idx: indicates which sample each row of model_out belongs to\n",
    "    noisy_x: noisy data\n",
    "    t: diffusion timestep\n",
    "    batch: noisy batch, ignored except by strange SDEs\n",
    "    \"\"\"\n",
    "    _, std = sde.marginal_prob(\n",
    "        x=torch.ones_like(model_out),\n",
    "        t=t,\n",
    "        batch_idx=batch_idx,\n",
    "        batch=batch,\n",
    "    )\n",
    "    # Note the slack tolerances in test_model_utils.py: the choice of ModelTarget does make a difference.\n",
    "    if model_target == ModelTarget.score_times_std:\n",
    "        return model_out / std\n",
    "    elif model_target == ModelTarget.logits:\n",
    "        # Not really a score, but logits will be handled downstream.\n",
    "        return model_out\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fn(x: T, t: torch.Tensor) -> T:\n",
    "    \"\"\"Calculate the score of a batch of data at a given timestep\n",
    "\n",
    "    Args:\n",
    "        x: batch of data\n",
    "        t: timestep\n",
    "\n",
    "    Returns:\n",
    "        score: score of the batch of data at the given timestep\n",
    "        注意训练时的score function已经乘了std\n",
    "    \"\"\"\n",
    "    model_out: T = model(x, t)\n",
    "    fns = {k: convert_model_out_to_score for k in corruption.sdes.keys()}\n",
    "\n",
    "    scores = apply(\n",
    "        fns=fns,\n",
    "        model_out=model_out,\n",
    "        broadcast=dict(t=t, batch=x),\n",
    "        sde=corruption.sdes,\n",
    "        model_target=model_target,\n",
    "        batch_idx=corruption._get_batch_indices(x),\n",
    "    )\n",
    "\n",
    "    return model_out.replace(**scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "class Corruption(abc.ABC):\n",
    "    \"\"\"Abstract base class for corruption processes\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def T(self) -> float:\n",
    "        \"\"\"End time of the corruption process.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def marginal_prob(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "        pass  # mean: (num_nodes, num_features), std (num_nodes,)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def prior_sampling(\n",
    "        self,\n",
    "        shape: Union[torch.Size, Tuple],\n",
    "        conditioning_data: Optional[BatchedData] = None,\n",
    "        batch_idx: B = None,  # This is normally unused but is needed for special cases such as sample-wise zero-centering.\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def prior_logp(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute log-density of the prior distribution.\n",
    "\n",
    "        Useful for computing the log-likelihood via probability flow ODE.\n",
    "\n",
    "        Args:\n",
    "          z: latent code\n",
    "        Returns:\n",
    "          log probability density\n",
    "        \"\"\"\n",
    "        pass  # prior_logp: (batch_size,)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample_marginal(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Sample marginal for x(t) given x(0).\n",
    "        Returns:\n",
    "          sampled x(t) (same shape as input x).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class SDE(Corruption):\n",
    "    \"\"\"Corruption using a stochastic differential equation.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sde(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns drift f and diffusion coefficient g such that dx = f * dt + g * sqrt(dt) * standard Gaussian\"\"\"\n",
    "        pass  # drift: (nodes_per_sample * batch_size, num_features), diffusion (batch_size,)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def marginal_prob(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns mean and standard deviation of the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n",
    "        pass  # mean: (nodes_per_sample * batch_size, num_features), std: (nodes_per_sample * batch_size, 1)\n",
    "\n",
    "    def mean_coeff_and_std(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns mean coefficient and standard deviation of marginal distribution at time t.\"\"\"\n",
    "        return self.marginal_prob(\n",
    "            torch.ones_like(x), t, batch_idx, batch\n",
    "        )  # mean_coeff: same shape as x, std: same shape as x\n",
    "\n",
    "    def sample_marginal(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Sample marginal for x(t) given x(0).\n",
    "        Returns:\n",
    "          sampled x(t)\n",
    "        \"\"\"\n",
    "        mean, std = self.marginal_prob(x=x, t=t, batch_idx=batch_idx, batch=batch)\n",
    "        z = torch.randn_like(x)\n",
    "\n",
    "        return mean + std * z\n",
    "\n",
    "class VESDE(SDE):\n",
    "    def __init__(self, sigma_min: float = 0.01, sigma_max: float = 50.0):\n",
    "        \"\"\"Construct a Variance Exploding SDE.\n",
    "\n",
    "        The marginal standard deviation grows exponentially from sigma_min to sigma_max.\n",
    "\n",
    "        Args:\n",
    "          sigma_min: smallest sigma.\n",
    "          sigma_max: largest sigma.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "\n",
    "    @property\n",
    "    def T(self) -> float:\n",
    "        return 1.0\n",
    "\n",
    "    def sde(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sigma = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n",
    "        drift = torch.zeros_like(x)\n",
    "        diffusion = maybe_expand(\n",
    "            sigma\n",
    "            * torch.sqrt(\n",
    "                torch.tensor(2 * (np.log(self.sigma_max) - np.log(self.sigma_min)), device=t.device)\n",
    "            ),\n",
    "            batch_idx,\n",
    "            x,\n",
    "        )\n",
    "        return drift, diffusion\n",
    "\n",
    "    def marginal_prob(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        std = maybe_expand(self.sigma_min * (self.sigma_max / self.sigma_min) ** t, batch_idx, x)\n",
    "        mean = x\n",
    "        return mean, std\n",
    "\n",
    "    def prior_sampling(\n",
    "        self,\n",
    "        shape: Union[torch.Size, Tuple],\n",
    "        conditioning_data: Optional[BatchedData] = None,\n",
    "        batch_idx: B = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return torch.randn(*shape) * self.sigma_max\n",
    "\n",
    "    def prior_logp(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        shape = z.shape\n",
    "        N = np.prod(shape[1:])\n",
    "        if batch_idx is not None:\n",
    "            return -N / 2.0 * np.log(2 * np.pi * self.sigma_max**2) - scatter_add(\n",
    "                torch.sum(z**2, dim=1), batch_idx\n",
    "            ) / (2 * self.sigma_max**2)\n",
    "        else:\n",
    "            return -N / 2.0 * np.log(2 * np.pi * self.sigma_max**2) - torch.sum(\n",
    "                z**2, dim=tuple(range(1, z.ndim))\n",
    "            ) / (2 * self.sigma_max**2)\n",
    "\n",
    "class WrappedSDEMixin:\n",
    "    def sample_marginal(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: torch.LongTensor = None,\n",
    "        batch: Optional[BatchedData] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        _super = super()\n",
    "        assert (\n",
    "            isinstance(self, SDE)\n",
    "            and hasattr(_super, \"sample_marginal\")\n",
    "            and hasattr(self, \"wrapping_boundary\")\n",
    "        )\n",
    "        if (x > self.wrapping_boundary).any() or (x < 0).any():\n",
    "            # Values outside the wrapping boundary are valid in principle, but could point to an issue in the data preprocessing,\n",
    "            # as typically we assume that the input data is inside the wrapping boundary (e.g., angles between 0 and 2*pi).\n",
    "            print(\"Warning: Wrapped SDE has received input outside of the wrapping boundary.\")\n",
    "        noisy_x = _super.sample_marginal(x=x, t=t, batch_idx=batch_idx, batch=batch)\n",
    "        return self.wrap(noisy_x)\n",
    "\n",
    "    def prior_sampling(\n",
    "        self,\n",
    "        shape: Union[torch.Size, Tuple],\n",
    "        conditioning_data: Optional[BatchedData] = None,\n",
    "        batch_idx: B = None,\n",
    "    ) -> torch.Tensor:\n",
    "        _super = super()\n",
    "        assert isinstance(self, SDE) and hasattr(_super, \"prior_sampling\")\n",
    "        return self.wrap(_super.prior_sampling(shape=shape, conditioning_data=conditioning_data))\n",
    "\n",
    "    def wrap(self, x):\n",
    "        assert isinstance(self, SDE) and hasattr(self, \"wrapping_boundary\")\n",
    "        return wrap(x, self.wrapping_boundary)\n",
    "\n",
    "class WrappedVESDE(WrappedSDEMixin, VESDE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        wrapping_boundary: float = 1.0,\n",
    "        sigma_min: float = 0.01,\n",
    "        sigma_max: float = 50.0,\n",
    "    ):\n",
    "        super().__init__(sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "        self.wrapping_boundary = wrapping_boundary\n",
    "\n",
    "class NumAtomsVarianceAdjustedWrappedVESDE(WrappedVESDE):\n",
    "    \"\"\"Wrapped VESDE with variance adjusted by number of atoms. We divide the standard deviation by the cubic root of the number of atoms.\n",
    "    The goal is to reduce the influence by the cell size on the variance of the fractional coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        wrapping_boundary: float | torch.Tensor = 1.0,\n",
    "        sigma_min: float = 0.01,\n",
    "        sigma_max: float = 5.0,\n",
    "        limit_info_key: str = \"num_atoms\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sigma_min=sigma_min, sigma_max=sigma_max, wrapping_boundary=wrapping_boundary\n",
    "        )\n",
    "        self.limit_info_key = limit_info_key\n",
    "\n",
    "    def std_scaling(self, batch: BatchedData) -> torch.Tensor:\n",
    "        return batch[self.limit_info_key] ** (-1 / 3)\n",
    "\n",
    "    def marginal_prob(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: BatchedData | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        mean, std = super().marginal_prob(x, t, batch_idx, batch)\n",
    "        assert (\n",
    "            batch is not None\n",
    "        ), \"batch must be provided when using NumAtomsVarianceAdjustedWrappedVESDEMixin\"\n",
    "        std_scale = self.std_scaling(batch)\n",
    "        std = std * maybe_expand(std_scale, batch_idx, like=std)\n",
    "        return mean, std\n",
    "\n",
    "    def prior_sampling(\n",
    "        self,\n",
    "        shape: torch.Size | tuple,\n",
    "        conditioning_data: BatchedData | None = None,\n",
    "        batch_idx=None,\n",
    "    ) -> torch.Tensor:\n",
    "        _super = super()\n",
    "        assert isinstance(self, DiffSDE) and hasattr(_super, \"prior_sampling\")\n",
    "        assert (\n",
    "            conditioning_data is not None\n",
    "        ), \"batch must be provided when using NumAtomsVarianceAdjustedWrappedVESDEMixin\"\n",
    "        num_atoms = conditioning_data[self.limit_info_key]\n",
    "        batch_idx = torch.repeat_interleave(\n",
    "            torch.arange(num_atoms.shape[0], device=num_atoms.device), num_atoms, dim=0\n",
    "        )\n",
    "        std_scale = self.std_scaling(conditioning_data)\n",
    "        # prior sample is randn() * sigma_max, so we need additionally multiply by std_scale to get the correct variance.\n",
    "        # We call VESDE.prior_sampling (a \"grandparent\" function) because the super() prior_sampling already does the wrapping,\n",
    "        # which means we couldn't do the variance adjustment here anymore otherwise.\n",
    "        prior_sample = DiffVESDE.prior_sampling(self, shape=shape).to(num_atoms.device)\n",
    "        return self.wrap(prior_sample * maybe_expand(std_scale, batch_idx, like=prior_sample))\n",
    "\n",
    "    def sde(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "        batch: BatchedData | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        sigma = self.marginal_prob(x, t, batch_idx, batch)[1]\n",
    "        sigma_min = self.marginal_prob(x, torch.zeros_like(t), batch_idx, batch)[1]\n",
    "        sigma_max = self.marginal_prob(x, torch.ones_like(t), batch_idx, batch)[1]\n",
    "        drift = torch.zeros_like(x)\n",
    "        diffusion = sigma * torch.sqrt(2 * (sigma_max.log() - sigma_min.log()))\n",
    "        return drift, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AncestralSamplingPredictor():\n",
    "    \"\"\"Suitable for all linear SDEs.\n",
    "\n",
    "    This predictor is derived by converting the score prediction to a prediction of x_0 given x_t, and then\n",
    "    sampling from the conditional distribution of x_{t-dt} given x_0 and x_t according to the corruption process.\n",
    "    It corresponds to equation (47) in Song et al. for VESDE (https://openreview.net/forum?id=PxTIG12RRHS)\n",
    "    and equation (7) in Ho et al. for VPSDE (https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "    In more detail: suppose the SDE has marginals x_t ~ N(alpha_t *x_0, sigma_t**2)\n",
    "\n",
    "    We estimate x_0 as follows:\n",
    "    x_0 \\approx (x_t + sigma_t^2 * score) / alpha_t\n",
    "\n",
    "    For any s < t, the forward corruption process implies that\n",
    "    x_t| x_s ~ N(alpha_t/alpha_s * x_s, sigma_t^2 - sigma_s^2 * alpha_t^2 / alpha_s^2)\n",
    "\n",
    "    Now go away and do some algebra to get the mean and variance of x_s given x_t\n",
    "    and x_0, and you will get the coefficients in the `update_given_score` method below.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def update_fn(\n",
    "        self,\n",
    "        *,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        dt: torch.Tensor,\n",
    "        batch_idx: torch.LongTensor,\n",
    "        batch: BatchedData | None,\n",
    "    ) -> SampleAndMean:\n",
    "        \"\"\"One update of the predictor.\n",
    "\n",
    "        Args:\n",
    "          x: current state\n",
    "          t: timesteps\n",
    "          batch_idx: indicates which sample each row of x belongs to\n",
    "\n",
    "        Returns:\n",
    "           (sampled next state, mean next state)\n",
    "        \"\"\"\n",
    "        assert self.score_fn is not None\n",
    "        score = self.score_fn(x=x, t=t, batch_idx=batch_idx)\n",
    "        return self.update_given_score(\n",
    "            x=x, t=t, dt=dt, batch_idx=batch_idx, score=score, batch=batch\n",
    "        )\n",
    "\n",
    "\n",
    "    def update_given_score(\n",
    "        self,\n",
    "        *,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        dt: torch.Tensor,\n",
    "        batch_idx: torch.LongTensor,\n",
    "        score: torch.Tensor,\n",
    "        batch: BatchedData | None,\n",
    "    ) -> SampleAndMean:\n",
    "        x_coeff, score_coeff, std = self._get_coeffs(\n",
    "            x=x,\n",
    "            t=t,\n",
    "            dt=dt,\n",
    "            batch_idx=batch_idx,\n",
    "            batch=batch,\n",
    "        )\n",
    "        # Sample random noise.\n",
    "        z = torch.randn_like(x_coeff)\n",
    "\n",
    "        mean = x_coeff * x + score_coeff * score\n",
    "        sample = mean + std * z\n",
    "\n",
    "        return sample, mean\n",
    "\n",
    "    def _get_coeffs(self, x, t, dt, batch_idx, batch):\n",
    "        \"\"\"\n",
    "        Compute coefficients for ancestral sampling.\n",
    "        This is in a separate method to make it easier to test.\"\"\"\n",
    "        sde = self.corruption\n",
    "        assert isinstance(sde, SDE)\n",
    "\n",
    "        # Previous timestep\n",
    "        s = t + dt\n",
    "\n",
    "        alpha_t, sigma_t = sde.mean_coeff_and_std(x=x, t=t, batch_idx=batch_idx, batch=batch)\n",
    "        if batch_idx is None:\n",
    "            is_time_zero = s <= 0\n",
    "        else:\n",
    "            is_time_zero = s[batch_idx] <= 0\n",
    "        alpha_s, sigma_s = sde.mean_coeff_and_std(x=x, t=s, batch_idx=batch_idx, batch=batch)\n",
    "        sigma_s[is_time_zero] = 0\n",
    "\n",
    "        # If you are trying to match this up with algebra in papers, it may help to\n",
    "        # notice that for VPSDE, sigma2_t_given_s == 1 - alpha_t_given_s**2, except\n",
    "        # that alpha_t_given_s**2 is clipped.\n",
    "        sigma2_t_given_s = sigma_t**2 - sigma_s**2 * alpha_t**2 / alpha_s**2\n",
    "        sigma_t_given_s = torch.sqrt(sigma2_t_given_s)\n",
    "        std = sigma_t_given_s * sigma_s / sigma_t\n",
    "\n",
    "        # Clip alpha_t_given_s so that we do not divide by zero.\n",
    "        min_alpha_t_given_s = 0.001\n",
    "        alpha_t_given_s = alpha_t / alpha_s\n",
    "        if torch.any(alpha_t_given_s < min_alpha_t_given_s):\n",
    "            # If this warning is raised, you probably should change something: either modify your noise schedule\n",
    "            # so that the diffusion coefficient does not blow up near sde.T, or only denoise from sde.T - eps,\n",
    "            # rather than sde.T.\n",
    "            alpha_t_given_s = torch.clip(alpha_t_given_s, min_alpha_t_given_s, 1)\n",
    "\n",
    "        score_coeff = sigma2_t_given_s / alpha_t_given_s\n",
    "\n",
    "        x_coeff = 1.0 / alpha_t_given_s\n",
    "\n",
    "        std[is_time_zero] = 0\n",
    "\n",
    "        return x_coeff, score_coeff, std\n",
    "    \n",
    "ASPridictor = AncestralSamplingPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "\n",
    "class ScoreFunction(Protocol):\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        batch_idx: B = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Calculate score.\n",
    "\n",
    "        Args:\n",
    "            x: Samples at which the score should be calculated. Shape [num_nodes, ...]\n",
    "            t: Timestep for each sample. Shape [num_samples,]\n",
    "            batch_idx: Indicates which sample each row of x belongs to. Shape [num_nodes,]\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LangevinCorrector():\n",
    "    def __init__(\n",
    "        self,\n",
    "        corruption: Corruption,\n",
    "        score_fn: ScoreFunction | None,\n",
    "        n_steps: int,\n",
    "        snr: float = 0.2,\n",
    "        max_step_size: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"The Langevin corrector.\n",
    "\n",
    "        Args:\n",
    "            corruption: corruption process\n",
    "            score_fn: score function\n",
    "            n_steps: number of Langevin steps at each noise level\n",
    "            snr: signal-to-noise ratio\n",
    "            max_step_size: largest coefficient that the score can be multiplied by for each Langevin step.\n",
    "        \"\"\"\n",
    "        self.corruption = corruption\n",
    "        self.score_fn = score_fn\n",
    "        self.n_steps = n_steps\n",
    "        self.snr = snr\n",
    "        self.max_step_size = torch.tensor(max_step_size)\n",
    "\n",
    "    @classmethod\n",
    "    def is_compatible(cls, corruption: Corruption):\n",
    "        return (\n",
    "            isinstance(corruption, (VESDE, BaseVPSDE))\n",
    "            and super().is_compatible(corruption)\n",
    "            and not isinstance(corruption, WrappedSDEMixin)\n",
    "        )\n",
    "\n",
    "    def update_fn(self, *, x, t, batch_idx, dt: torch.Tensor) -> SampleAndMean:\n",
    "        assert self.score_fn is not None, \"Did you mean to use step_given_score?\"\n",
    "        for _ in range(self.n_steps):\n",
    "            score = self.score_fn(x, t, batch_idx)\n",
    "            x, x_mean = self.step_given_score(x=x, batch_idx=batch_idx, score=score, t=t, dt=dt)\n",
    "\n",
    "        return x, x_mean\n",
    "\n",
    "    def get_alpha(self, t: torch.FloatTensor, dt: torch.FloatTensor) -> torch.Tensor:\n",
    "        sde = self.corruption\n",
    "\n",
    "        if isinstance(sde, VPSDE):\n",
    "            alpha_bar = sde._marginal_mean_coeff(t) ** 2\n",
    "            alpha_bar_before = sde._marginal_mean_coeff(t + dt) ** 2\n",
    "            alpha = alpha_bar / alpha_bar_before\n",
    "        else:\n",
    "            alpha = torch.ones_like(t)\n",
    "        return alpha\n",
    "\n",
    "    def step_given_score(\n",
    "        self, *, x, batch_idx: torch.LongTensor | None, score, t: torch.Tensor, dt: torch.Tensor\n",
    "    ) -> SampleAndMean:\n",
    "        alpha = self.get_alpha(t, dt=dt)\n",
    "        snr = self.snr\n",
    "        noise = torch.randn_like(score)\n",
    "        grad_norm_square = torch.square(score).reshape(score.shape[0], -1).sum(dim=1)\n",
    "        noise_norm_square = torch.square(noise).reshape(noise.shape[0], -1).sum(dim=1)\n",
    "        if batch_idx is None:\n",
    "            grad_norm = grad_norm_square.sqrt().mean()\n",
    "            noise_norm = noise_norm_square.sqrt().mean()\n",
    "        else:\n",
    "            grad_norm = torch.sqrt(scatter_add(grad_norm_square, dim=-1, index=batch_idx)).mean()\n",
    "\n",
    "            noise_norm = torch.sqrt(scatter_add(noise_norm_square, dim=-1, index=batch_idx)).mean()\n",
    "\n",
    "        # If gradient is zero (i.e., we are sampling from an improper distribution that's flat over the whole of R^n)\n",
    "        # the step_size blows up. Clip step_size to avoid this.\n",
    "        # The EGNN reports zero scores when there are no edges between nodes.\n",
    "        step_size = (snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n",
    "        step_size = torch.minimum(step_size, self.max_step_size)\n",
    "        step_size[grad_norm == 0, :] = self.max_step_size\n",
    "\n",
    "        # Expand step size to batch structure (score and noise have the same shape).\n",
    "        step_size = maybe_expand(step_size, batch_idx, score)\n",
    "\n",
    "        # Perform update, using custom update for SO(3) diffusion on frames.\n",
    "        mean = x + step_size * score\n",
    "        x = mean + torch.sqrt(step_size * 2) * noise\n",
    "\n",
    "        return x, mean\n",
    "\n",
    "LCorrector = LangevinCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedCorrectorMixin:\n",
    "    \"\"\"A mixin for wrapping the corrector in a WrappedSDE.\"\"\"\n",
    "\n",
    "    def step_given_score(\n",
    "        self,\n",
    "        *,\n",
    "        x: torch.Tensor,\n",
    "        batch_idx: torch.LongTensor,\n",
    "        score: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        dt: torch.Tensor,\n",
    "    ) -> SampleAndMean:\n",
    "        # mypy\n",
    "        assert isinstance(self, LangevinCorrector)\n",
    "        _super = super()\n",
    "        assert hasattr(_super, \"step_given_score\")\n",
    "        assert hasattr(self, \"corruption\") and hasattr(self.corruption, \"wrap\")\n",
    "        sample, mean = _super.step_given_score(x=x, score=score, t=t, batch_idx=batch_idx, dt=dt)\n",
    "        return wrap(sample), wrap(mean)\n",
    "\n",
    "\n",
    "class WrappedPredictorMixin:\n",
    "    \"\"\"A mixin for wrapping the predictor in a WrappedSDE.\"\"\"\n",
    "\n",
    "    def update_given_score(\n",
    "        self,\n",
    "        *,\n",
    "        x: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        dt: torch.Tensor,\n",
    "        batch_idx: torch.LongTensor,\n",
    "        score: torch.Tensor,\n",
    "        batch: Optional[BatchedData],\n",
    "    ) -> SampleAndMean:\n",
    "\n",
    "        _super = super()\n",
    "\n",
    "        sample, mean = _super.update_given_score(\n",
    "            x=x, t=t, dt=dt, batch_idx=batch_idx, score=score, batch=batch\n",
    "        )\n",
    "        return self.corruption.wrap(sample), self.corruption.wrap(mean)\n",
    "\n",
    "\n",
    "class WrappedAncestralSamplingPredictor(\n",
    "    WrappedPredictorMixin, AncestralSamplingPredictor\n",
    "):\n",
    "    @classmethod\n",
    "    def is_compatible(cls, corruption: Corruption):\n",
    "        return isinstance(corruption, (VESDE)) and isinstance(\n",
    "            corruption, WrappedSDEMixin\n",
    "        )\n",
    "\n",
    "\n",
    "class WrappedLangevinCorrector(WrappedCorrectorMixin, LangevinCorrector):\n",
    "    @classmethod\n",
    "    def is_compatible(cls, corruption: Corruption):\n",
    "        return isinstance(corruption, (VESDE)) and isinstance(\n",
    "            corruption, WrappedSDEMixin\n",
    "        )\n",
    "    \n",
    "WLCorrector = WrappedLangevinCorrector(LCorrector)\n",
    "WASPredictor = WrappedAncestralSamplingPredictor(ASPridictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_both(\n",
    "    *, sample_and_mean: Tuple[torch.Tensor, torch.Tensor], old_x: torch.Tensor, mask: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    return tuple(_mask(old_x=old_x, new_x=x, mask=mask) for x in sample_and_mean)  # type: ignore\n",
    "\n",
    "\n",
    "def mask_replace(\n",
    "    samples_means: dict[str, Tuple[torch.Tensor, torch.Tensor]],\n",
    "    batch: BatchedData,\n",
    "    mean_batch: BatchedData,\n",
    "    mask: dict[str, torch.Tensor | None],\n",
    ") -> SampleAndMean:\n",
    "    # Apply masks\n",
    "    samples_means = apply(\n",
    "        fns={k: mask_both for k in samples_means},\n",
    "        broadcast={},\n",
    "        sample_and_mean=samples_means,\n",
    "        mask=mask,\n",
    "        old_x=batch,\n",
    "    )\n",
    "\n",
    "    # Put the updated values in `batch` and `mean_batch`\n",
    "    batch = batch.replace(**{k: v[0] for k, v in samples_means.items()})\n",
    "    mean_batch = mean_batch.replace(**{k: v[1] for k, v in samples_means.items()})\n",
    "    return batch, mean_batch\n",
    "\n",
    "def _get_batch_indices(self, batch: Diffusable) -> Dict[str, torch.Tensor]:\n",
    "    return {k: batch.get_batch_idx(k) for k in self.corrupted_fields}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoise(\n",
    "    batch: Diffusable,\n",
    "    record: bool = False,\n",
    "    max_t=1000, \n",
    "    eps_t=1, \n",
    "    device=torch.device(\"cpu\"),\n",
    "    N=50,\n",
    "    n_steps_corrector=1,\n",
    ") -> SampleAndMeanAndMaybeRecords:\n",
    "    \"\"\"Denoise from a prior sample to a t=eps_t sample.\"\"\"\n",
    "\n",
    "    mean_batch = batch.clone()\n",
    "\n",
    "    # Decreasing timesteps from T to eps_t\n",
    "    timesteps = torch.linspace(max_t, eps_t, N, device=device)\n",
    "    dt = -torch.tensor((max_t - eps_t) / (N - 1)).to(device)\n",
    "\n",
    "    for i in tqdm(range(N), miniters=50, mininterval=5):\n",
    "        # Set the timestep\n",
    "        t = torch.full((batch.get_batch_size(),), timesteps[i], device=device)\n",
    "\n",
    "        # Corrector updates.\n",
    "        for _ in range(n_steps_corrector):\n",
    "            score = score_fn(batch, t)\n",
    "\n",
    "            samples_means = WLCorrector.step_given_score(\n",
    "                x=batch,\n",
    "                t=t,\n",
    "                dt=dt,\n",
    "                batch_idx=_get_batch_indices(batch),\n",
    "                score=score,\n",
    "            )\n",
    "\n",
    "        # Predictor updates\n",
    "        score = score_fn(batch, t)\n",
    "\n",
    "        samples_means = WASPredictor.update_given_score(x=batch, t=t, dt=dt, batch_idx=_get_batch_indices(batch), score=score)\n",
    "        \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
